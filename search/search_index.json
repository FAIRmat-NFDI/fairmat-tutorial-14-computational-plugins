{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Developing schemas and parsers for FAIR computational data storage using NOMAD-Simulations","text":"<p>This tutorial will provide foundational knowledge for customizing NOMAD to fit the specific needs of your computational research project.</p>"},{"location":"#tutorial-information-and-preparation","title":"Tutorial information and preparation","text":"<p>The FAIRmat Tutorial 14, presented by FAIRmat Area C Computation, took place on Wednesday, July 10, 2024.</p> <p>A recording of the tutorial is available on the FAIRmat and NOMAD YouTube channel</p> <p>The focus of this tutorial is the <code>nomad-simulations</code> schema package, which can be installed as follows:</p> <p>First, create the directory and the virtual environment in the terminal. Note that the Python version must be 3.9-3.11 for <code>nomad-simulations</code> to work:</p> <pre><code>mkdir test_nomadsimulations\ncd test_nomadsimulations/\npython3.11 -m venv .pyenv\nsource .pyenv/bin/activate\n</code></pre> <p>Once this is done, we can pip install the <code>nomad-simulations</code> package:</p> <pre><code>pip install --upgrade pip\npip install nomad-simulations --index-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi/simple\n</code></pre> <p>We hope that this tutorial helps you to leverage NOMAD to enhance your own research! If you have any questions or would like to suggest future development directions, please come talk to us on the NOMAD Discord Server under the \"Computation\" section.</p>"},{"location":"custom_workflows/","title":"Part 5 - Interfacing complex simulation and analysis workflows with NOMAD.","text":"<p>The concept of workflow is essential in simulations and analysis of Materials Science and Chemistry data. In NOMAD, we give support to workflows and understand them as a way of organizing data to keep the full provenance of an activity (either experimental or computational). Workflows can be also parsed in NOMAD in two slightly different ways: in an automatic way or by defining a custom workflow additional file. The first case is done when the simulation input and output files contain enough information to recognize a certain workflow, and it is typically done in the parser plugin. You can read more on how to prepare your input/output files to make this automatic recognition easier in the Extra: Standard workflows recognition section at the end of this page.</p> <p>This part of the Tutorial will show you the basic concepts of workflows in NOMAD, as well as how to define your own custom workflow. For that, we will use a ficticious example of a simulation workflow, where the files and folder structure is: <pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.h5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.h5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u2514\u2500\u2500 pressure2\n \u00a0\u00a0 \u251c\u2500\u2500 temperature1\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.h5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 temperature2\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.h5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n \u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n \u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n</code></pre> Each of the files in this folder tree represents an electronic-structure calculation (either DFT, TB, or DMFT), which in turn is then parsed into a singular entry in NOMAD by matching and parsing the files as explained in Part IV - Parser Plugins. This folder structure presents a typical workflow calculation which can be represented as a provenance graph:</p> <p>Note that the arrows here indicate directionality of the nodes inputs, tasks (DFT, TB, DMFT), and outputs. Here, input refers to the all input information given to perform the calculation. In Part II - NOMAD-Simulations, we saw that the input is further divided in the <code>ModelSystem</code> information (i.e., atom positions, cell information, etc.) and the <code>ModelMethod</code> information (i.e., the mathematical model and numerical parameters). <code>DFT</code>, <code>TB</code> and <code>DMFT</code> refer to individual tasks of the workflow, and refers to the activity Simulation. Output refers to the output data of each of the final DMFT tasks and is directly related with the <code>Outputs</code> section defined in Part II - NOMAD-Simulations.</p> <p>Go to the NOMAD Upload page, create a new upload, and drag-and-drop the zipped <code>example_files.zip</code> file. This action should generate 8 entries in total:</p> <p>The assignements for this part of the tutorial are about setting manually the following workflows:</p> <ol> <li>A <code>SinglePoint</code> workflow for one of the calculations (e.g., the DFT one) in the <code>pressure1</code> subfolder.</li> <li>An overarching workflow entry for each pressure P<sub>i=1,2</sub>, grouping all <code>SinglePoint</code> DFT, TB, DMFT at T<sub>1</sub>, and DMFT at T<sub>2</sub> tasks.</li> <li>A top level workflow entry, grouping together all pressure calculations.</li> <li>Use or define a new plugin to run functionalities and normalizations for the custom workflows.</li> </ol> <p>The solutions to these assignements can be found in the Workflow YAML files. We recommend you to try writing these files yourself first, and then compare them with the tested files.</p>"},{"location":"custom_workflows/#assignement-51-singlepoint-workflow","title":"Assignement 5.1: <code>SinglePoint</code> workflow","text":"<p>NOMAD is able to recognize certain workflows in an automatic way, such as the <code>SinglePoint</code> case mentioned above. However, to showcase how to the use workflows in NOMAD, you will learn how to manually construct the <code>SinglePoint</code> workflow, represented by the following provenance graph:</p> <p>To define a workflow manually in NOMAD, you must add a YAML file to the upload folder. This file contains references to the relevant inputs, outputs, and tasks sections. This file should be named <code>&lt;filename&gt;.archive.yaml</code>. In this case, you should include the file <code>single_point.archive.yaml</code> with the following content:</p> <pre><code>workflow2:\n  name: SinglePoint\n  m_def: simulationworkflowschema.general.SimulationWorkflow\n  inputs:\n    - name: Input structure\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n  outputs:\n    - name: Output calculation\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\n      name: DFT at Pressure P1\n      inputs:\n        - name: Input structure\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n      outputs:\n        - name: Output calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>Note several things about the content of this file:</p> <ol> <li><code>name</code> keys are optional.</li> <li><code>m_def</code> defines the section definition (in this case, the <code>SimulationWorkflow</code> section defined in the simulationworkflowschema plugin and the <code>TaskReference</code> section defined in the source code of NOMAD) used for this workflow. This allows us to use the <code>normalize()</code> functions defined in this class. See Assignement 5.4: Extending workflows plugins and <code>m_def</code> in the custom workflow schema for more information</li> <li>The root path of the upload can be referenced with <code>../upload/archive/mainfile/</code>. Starting from there, the original directory tree structure of the upload is maintained. There are other formats for referencing, and you can find more information on the corresponding general NOMAD documentation page</li> <li><code>inputs</code> reference the section containing inputs of the whole workflow. In this case this is the section <code>run[0].system[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>outputs</code> reference the section containing outputs of the whole workflow. In this case this is the section <code>run[0].calculation[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>tasks</code> reference the section containing tasks of each step in the workflow. These must also contain <code>inputs</code> and <code>outputs</code> properly referencing the corresponding sections; this will then link inputs/outputs/tasks in the NOMAD Archive. In this case this is a <code>TaskReference</code> to the section <code>workflow2</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>section</code> reference to the uploaded mainfile specific section. The left side of the <code>#</code> symbol contains the path to the mainfile, while the right contains the path to the section.</li> </ol> <code>run</code> and <code>data</code> sections in the NOMAD entries <p>As we explained in Part I - Intro to NOMAD, we are currently migrating the section definitions in <code>run</code> to <code>data</code>. In the Part II - NOMAD-Simulations, we showed you the definitions that populate the <code>data</code> section. However, the current parsers in NOMAD still use the legacy section <code>run</code>, and that is why we need to still use references to these sections. Nevertheless, the structure of the legacy <code>run</code> schema and the new <code>data</code> schema is the same: in the legacy schema we had the <code>System</code> - <code>Method</code> - <code>Calculation</code> sub-sections, while in the new schema we have <code>ModelSystem</code> - <code>ModelMethod</code> - <code>Outputs</code>, and the very same reasoning will apply once we finish the migration.</p> <p>Drag this new file into the upload we created. This will produce an extra entry with the following Overview content:</p> <p>Note that you are referencing sections which are lists. Thus, in each case you should be careful to reference the correct section for inputs and outputs (example: a <code>GeometryOptimization</code> workflow calculation will have the \"Input structure\" as <code>run[0].system[0]</code>, while the \"Output calculation\" would also contain <code>run[0].system[-1]</code>, and all intermediate steps must input/output the corresponding section to be linked).</p> NOMAD workflow filename <p>The NOMAD workflow YAML file name, i.e., <code>&lt;filename&gt;</code> in the explanation above, can be any custom name defined by the user, but the file must keep the extension <code>.archive.yaml</code> at the end. This is done in order for NOMAD to recognize this file as a custom schema. Custom schemas are widely used in experimental parsing, and you can learn more about them in the FAIRmat tutorial 8.</p> <p>You can extend the workflow meta-information by adding the metholodogical input parameters as stored in the section path <code>run[0].method[-1]</code>. The new <code>single_point.archive.yaml</code> will be:</p> <pre><code>workflow2:\n  name: SinglePoint\n  m_def: simulationworkflowschema.general.SimulationWorkflow\n  inputs:\n    - name: Input structure\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n    - name: Input methodology parameters\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\n  outputs:\n    - name: Output calculation\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\n      name: DFT at Pressure P1\n      inputs:\n        - name: Input structure\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n        - name: Input methodology parameters\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\n      outputs:\n        - name: Output calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>which in turn produces a similar workflow than before, but with an extra input node:</p>"},{"location":"custom_workflows/#assignement-52-pressure-workflows","title":"Assignement 5.2: Pressure workflows","text":"<p>Now that you know the basics of the workflow YAML schema, let's try to define an overarching workflow for each of the pressures. For this section, you will learn how to create the workflow YAML schema for the P<sub>1</sub> case; the extension for P<sub>2</sub> is then a matter of changing names and paths in the YAML files. For simplicity, we will skip referencing to methodology sections.</p> <p>Thus, the <code>inputs</code> can be defined as: <pre><code>workflow2:\n  name: DFT+TB+DMFT at P1\n  m_def: simulationworkflowschema.general.SimulationWorkflow\n  inputs:\n    - name: Input structure\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n</code></pre> and there are two <code>outputs</code>, one for each of the DMFT calculations at distinct temperatures: <pre><code>  outputs:\n    - name: Output DMFT at P1, T1 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.h5#/run/0/calculation/-1'\n    - name: Output DMFT at P1, T2 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.h5#/run/0/calculation/-1'\n</code></pre> Now, <code>tasks</code> are defined for each of the activities performed (each corresponding to an underlying SinglePoint workflow). To define a linked workflow as it is the case, each task must contain an input that corresponds to one of the outputs of the previous task. Moreover, the first task should take as input the overall input of the workflow, and the final task should also have as an output the overall workflow output. Then: <pre><code>  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\n      name: DFT at P1\n      inputs:\n        - name: Input structure\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n      outputs:\n        - name: Output DFT at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/tb_p1.wout#/workflow2'\n      name: TB at P1\n      inputs:\n        - name: Input DFT at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n      outputs:\n        - name: Output TB at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.h5#/workflow2'\n      name: DMFT at P1 and T1\n      inputs:\n        - name: Input TB at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n      outputs:\n        - name: Output DMFT at P1, T1 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.h5#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.h5#/workflow2'\n      name: DMFT at P1 and T2\n      inputs:\n        - name: Input TB at P1 calculation\n          section: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n      outputs:\n        - name: Output DMFT at P1, T2 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.h5#/run/0/calculation/-1'\n</code></pre> Note here:</p> <ul> <li>The <code>inputs</code> for each subsequent step are the <code>outputs</code> of the previous step.</li> <li>The final two <code>outputs</code> coincide with the <code>workflow2</code> <code>outputs</code>.</li> </ul> <p>This workflow (<code>pressure1.archive.yaml</code>) file will then produce an entry with the following Overview page:</p> <p>Similarly, for P<sub>2</sub> you can upload a new <code>pressure2.archive.yaml</code> file with the same content, except when substituting 'pressure1' and 'p1' by their counterparts. This will produce a similar graph than the one showed before but for \"P2\".</p>"},{"location":"custom_workflows/#assignement-53-the-top-level-workflow","title":"Assignement 5.3: The top-level workflow","text":"<p>After adding the workflow YAML files, Your upload folder directory now looks like: <pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.h5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.h5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure1.archive.yaml\n\u251c\u2500\u2500 pressure2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.h5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.h5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure2.archive.yaml\n\u2514\u2500\u2500 single_point.archive.yaml\n</code></pre> In order to define the general workflow that groups all pressure calculations, you can reference directly the previous <code>pressure1.archive.yaml</code> and <code>pressure2.archive.yaml</code> files as tasks. Still, <code>inputs</code> and <code>outputs</code> must be referenced to their corresponding file and section paths.</p> <p>Create a new <code>fullworkflow.archive.yaml</code> file with the <code>inputs</code>: <pre><code>workflow2:\n  name: Full calculation at different pressures for SrVO3\n  m_def: simulationworkflowschema.general.SimulationWorkflow\n  inputs:\n    - name: Input structure at P1\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n    - name: Input structure at P2\n      section: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\n</code></pre> And <code>outputs</code>: <pre><code>  outputs:\n    - name: Output DMFT at P1, T1 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.h5#/run/0/calculation/-1'\n    - name: Output DMFT at P1, T2 calculation\n      section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.h5#/run/0/calculation/-1'\n    - name: Output DMFT at P2, T1 calculation\n      section: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.h5#/run/0/calculation/-1'\n    - name: Output DMFT at P2, T2 calculation\n      section: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.h5#/run/0/calculation/-1'\n</code></pre> Finally, <code>tasks</code> references the previous YAML schemas as follows: <pre><code>  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1.archive.yaml#/workflow2'\n      name: DFT+TB+DMFT at P1\n      inputs:\n        - name: Input structure at P1\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n      outputs:\n        - name: Output DMFT at P1, T1 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.h5#/run/0/calculation/-1'\n        - name: Output DMFT at P1, T2 calculation\n          section: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.h5#/run/0/calculation/-1'\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure2.archive.yaml#/workflow2'\n      name: DFT+TB+DMFT at P2\n      inputs:\n        - name: Input structure at P2\n          section: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\n      outputs:\n        - name: Output DMFT at P2, T1 calculation\n          section: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.h5#/run/0/calculation/-1'\n        - name: Output DMFT at P2, T2 calculation\n          section: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.h5#/run/0/calculation/-1'\n</code></pre></p> <p>This will produce the following entry and its Overview page:</p>"},{"location":"custom_workflows/#assignement54","title":"Assignement 5.4: Extending workflows plugins and <code>m_def</code> in the custom workflow schema","text":"<p>In the previous assignements, you learn how to define your own workflows. An important step in the definition of this YAML files is the specification of <code>m_def</code>. This key allows us automate calls for the <code>normalize()</code> function of the specified class (in all the examples above, these were <code>SimulationWorkflow</code> and <code>TaskReference</code>). This means we can extract more information from this workflows if <code>m_def</code> is defined as a standard workflow section, e.g., <code>SinglePoint</code> or <code>GeometryOptimization</code>.</p> Current status of standard workflow definitions <p>We are currently working on extending the support for a large variety of standard workflows. For version 0.0.2, we have the <code>nomad-simulations</code> and the <code>simulationworkflowschema</code> plugins living in two separate Github repositories, but we are planning to merge both plugins into the <code>nomad-simulations</code> package. In order to see the current supported standard workflows, we recommend you to visit the simulationworkflowschema plugin and check its modules.</p> <p>In our previous example, we can standardize the definition of the <code>single_point.archive.yaml</code> by changing <code>m_def</code> to point to the standard <code>SinglePoint</code> workflow: <pre><code>workflow2:\n  name: SinglePoint\n  m_def: simulationworkflowschema.single_point.SinglePoint\n  inputs:\n    - name: Input structure\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n    - name: Input methodology parameters\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\n  outputs:\n    - name: Output calculation\n      section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n  tasks:\n    - m_def: nomad.datamodel.metainfo.workflow.TaskReference\n      task: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\n      name: DFT at Pressure P1\n      inputs:\n        - name: Input structure\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n        - name: Input methodology parameters\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\n      outputs:\n        - name: Output calculation\n          section: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre></p> <p>This has no practical effect, but it actually shows that if new methods/class functions are implemented in <code>SinglePoint</code> and called in <code>SinglePoint.normalize()</code> we could extract more information into the workflow entry and even display some derived properties.</p>"},{"location":"custom_workflows/#standard-workflows-recognition","title":"Extra: Standard workflows recognition","text":"<p>There are some cases where the NOMAD infrastructure is able to recognize certain workflows automatically when processing the uploaded files. The simplest example is any <code>SinglePoint</code> calculation, as explained above. Other examples include <code>GeometryOptimization</code>, <code>Phonons</code>, <code>DFT+GW</code>, and <code>MolecularDynamics</code>. Automated workflow detection may require your folder structure to fulfill certain conditions.</p> <p>Here are some general guidelines or suggestions for preparing your upload folder in order to make it easier for the automatic workflow recognition to work:</p> <ul> <li>Always organize your files in an top-down structure, i.e., the initial tasks should be upper in the directory tree, while the later tasks lower on it.</li> <li>Avoid having to go up and down between folders if some properties are derived between these files. These situations are very complicated to predict a priori in an automatic way.</li> <li>Keep as much information as possible regarding relative file paths, tasks, etc, in the output of your simulations.</li> <li>Avoid duplication of files in subfolders. If initially you do a simulation A from which a later simulation B is derived and you want to store B in a subfolder, there is no need to copy the A files inside the subfolder B.</li> </ul> <p>The folder structure used throughout this part is a good example of a clean upload which is friendly and easy to work with when defining NOMAD workflows.</p>"},{"location":"intro/","title":"Part 1 - Introduction to the NOMAD software and repository","text":"<p>This section of the tutorial consists of a slide presentation: Introduction Talk Slides</p> <p>For future use, we also include some important concepts below.</p>"},{"location":"intro/#background","title":"Background","text":"<p>NOMAD is an open-source, community-driven data infrastructure, focusing on materials science data. Originally built as a repository for data from DFT calculations, the NOMAD software can automatically extract data from the output of a large variety of simulation codes. Our previous computation-focused tutorials (CECAM workshop, Tutorial 10, and Tutorial 7) have highlighted the extension of NOMAD\u2019s functionalities to support advanced many-body calculations, classical molecular dynamics simulations, and complex simulation workflows.</p> <p>The key advantages of the NOMAD schema are summed up in FAIRmat's core values:</p> <ul> <li>Findable: a wide selection of the extracted data is indexed in a database, powering a the search with highly customizable queries and modular search parameters.</li> <li>Accessible: the same database specifies clear API and GUI protocols on how retrieve the full data extracted.</li> <li>Interoperable: we have a diverse team of experts who interface with various materials science communities, looking into harmonizing data representations and insights among them. Following the NOMAD standard also opens up the (meta)data to the \"NOMAD apps\" ecosystem. </li> <li>Reproducible: data is not standalone, but has a history, a vision, a workflow behind it. Our schema aims to capture the full context necessary for understanding and even regenerating via metadata.</li> </ul>"},{"location":"intro/#modular-design-and-plugins","title":"Modular Design and Plugins","text":"<p>Historically, the NOMAD parsers have always come packaged in a neat bundle. As interest in research data management has become more widespread and recognized across scientific disciplines, NOMAD has adapted along with those interests. Nowadays, in addition to the central NOMAD service, there are many standalone NOMAD Oasis deployments geared towards the needs of individual institutes or research groups.</p> <p>As such, NOMAD is shifting to a plugin model, where each deployment administrator can customize their setup. This permits a light-weight base installation and effective data processing pipelines. Conversely, it also lowers the threshold for whomever wants to contribute to this open-source community: you can focus on just a single schema or parser. The possibilities are as wide as you can make them, we just facilitate the common basis.</p> <p>The remainder of the tutorial will provide you with the basic knowledge for starting to develop your own computational parsers and schemas.</p>"},{"location":"mapping_annotations_protocol/","title":"Protocol for using <code>MappingAnnotationModel</code>","text":"<p>The following is a continuation of the section Mapping Annotations on the Schema side. It focus on building out a protocol for annotating the parser schema by exploring case scenarios. This has not yet been released to the public and the same disclaimer applies:</p> <p>DISCLAIMER: the code and functionalities covered in this section are still under construction and only serves as an illustration of the underlying concepts. Stay tuned for updates.</p>"},{"location":"mapping_annotations_protocol/#mapping-annotations-on-the-schema-side","title":"Mapping Annotations on the Schema side","text":"<p>...</p> <p>Many more approaches are at play in this example. Check out the \"Extra\" sections below for a more in-depth explanation and edge cases.</p>"},{"location":"mapping_annotations_protocol/#extra-exploring-mapping-annotations","title":"Extra: exploring Mapping Annotations","text":"<p><code>MappingAnnotationModel</code> itself accepts one of two argument keys:</p> <ul> <li><code>path</code>: specifies the node sequence leading up to the source node of interest. It leverages the JMESPath DSL for this, though other languages may be incorporated here in the future.</li> <li><code>operator</code>: the more active counterpart to <code>path</code>. Inserts a function (name in string format) between the source and target nodes. Source data are passed along in an array of <code>path</code>s matching the positional arguments, similar to Python <code>*args</code>.</li> </ul> <p>Operators, are the main tool for the parser developer to perform some data manipulation. The preferred approach is to keep them simple, just as <code>@staticmethod</code> functions under the parser class.   If they are short and one-of, they may even be defined as <code>lambda</code> functions. For anything with more complexity, we suggest extending the schema.</p>"},{"location":"mapping_annotations_protocol/#extra-defining-a-path","title":"Extra: defining a Path","text":"<p>Paths do not have to be absolute, i.e. starting from the root node, and may be written relative to the previous target/NOMAD parent node. The relative notation is highly encouraged for performance reasons. This is denoted in JMESPath by starting with the connector symbol (<code>.</code>). JMESPath also provides rich filtering features </p> <ul> <li>to select nodes by key name (starting with <code>@</code>). This is useful when presented with multiple named nodes at the same level.</li> <li>to select nodes / values by index for extracting anonymous nodes at the same level / tensors.</li> <li>to cycle between multiple branching scenarios via an <code>if-else</code> (<code>||</code>) logic.  </li> </ul>"},{"location":"mapping_annotations_protocol/#extra-aligning-source-and-target-segments","title":"Extra: aligning Source and Target Segments","text":"<p>Each path segment in the target/NOMAD schema (reference node -&gt; next node) has to be compared individually with its source counterpart. File formats may namely differ at any point in their level of semantic distinction. We encounter three possibilities:</p> <ul> <li>length source path segment = length target path segment: this case is straightforward and covered by the JMESPath features listed above.</li> <li>length source path segment &gt; length target path segment = 1: this is also completely covered by <code>path</code>.</li> <li>length source path segment = 1 &lt; length target path segment: at every non-matching target node, fall back on the trivial relative path <code>.</code> until the path segments line up again. </li> </ul>"},{"location":"nomad_simulations/","title":"Part 2 - Working with the NOMAD-Simulations schema plugin","text":"<p>In NOMAD, all the simulation metadata is defined under the <code>Simulation</code> section. You can find its Python schema defined in the <code>nomad-simulations</code> repository. The entry point for the schema is defined in src/nomad_simulations/schema_packages/__init__.py module. This section will appear under the <code>data</code> section for each NOMAD entry. There is also a specialized documentation page in the <code>nomad-simulations</code> repository.</p> <p>The <code>Simulation</code> section inherits from a more abstract section or concept called <code>BaseSimulation</code>, which at the same time inherits from another section, <code>Activity</code>.</p> Inheritance and composition <p>During this part, we will identify the is a concept with inheritance of one section into another (e.g., a <code>Simulation</code> is an <code>Activity</code>) and the has a concept with composition of one section under another (e.g., a <code>Simulation</code> has a <code>ModelSystem</code> sub-section). Strictly speaking, this equivalency is not entirely true, as we are loosing it in some cases. But for the purpose of learning the complicated rules of inheritance and composition, we will conceptually maintain this equivalency during this Tutorial.</p> <p>A set of base sections derived from the Basic Formal Ontology (BFO) is used as the basis for our section definitions. The previous inheritance allows us to define <code>Simulation</code> at the same level of other activities in Materials Science, e.g., <code>Experiment</code>, <code>Measurement</code>, <code>Analysis</code>. We do this in order to achieve a common vocabulary and data standardization with the experimental community. The relationship tree from the most abstract sections upon reaching <code>Simulation</code> is thus:</p> <p>Note that the white-headed arrow here indicates inheritance / is a relationship. <code>BaseSimulation</code> contains the general information about the <code>Program</code> used (see Program), as well as general time information about the simulation, e.g., the datetime at which it started (<code>datetime</code> is defined within <code>Activity</code> and is inherited by <code>BaseSimulation</code>) and ended (<code>datetime_end</code>). <code>Simulation</code> contains further information about the specific input and output sections (see below).</p> Notation for the section attributes in the UML diagram <p>Throughout this documentation page we will use UML diagrams to describe our section / class definitions, as well as to include the information abpit their attributes / quantities including their main definitions. The notation is:</p> <pre><code>&lt;name-of-quantity&gt;: &lt;type-of-quantity&gt;, &lt;(optional) units-of-quantity&gt;\n</code></pre> <p>Thus, <code>cpu1_start: np.float64, s</code> means that there is a quantity named <code>'cpu1_start'</code> of type <code>numpy.float64</code> and whose units are <code>'s'</code> (seconds). We also include the existence of sub-sections by bolding the name. For example, there is a sub-section under <code>Simulation</code> named <code>'model_method'</code> whose section defintion can be found in the <code>ModelMethod</code> section. We will represent this sub-section containment in more complex UML diagrams in the future using the containment arrow (see below for the specific ase of <code>Program</code>).</p> <p>We use double inheritance from <code>EntryData</code> in order to populate the <code>data</code> section in the NOMAD archive. All of the base sections discussed here are subject to the public normalize function in NOMAD. The private function <code>set_system_branch_depth()</code> is related with the ModelSystem section.</p> <p>Let's use this knowledge to see how to work with the schema in practice. If you have not already installed <code>nomad-simulations</code>, do so now following the instructions in the Overview.</p> <p>Assignment 2.1</p> <p>Create an instance of the <code>Simulation</code> section. Imagine you know that the CPU1 took 24 minutes and 30 seconds on finishing the simulation; can you populate the <code>Simulation</code> section with these times? What is the elapsed time in seconds? And in hours?</p> Solution 2.1 <p>We can open a Python console: <pre><code>python\n</code></pre></p> <p>And import and instantiate the <code>Simulation</code> section: <pre><code>from nomad_simulations.schema_packages.general import Simulation\nsimulation = Simulation()\n</code></pre></p> <p>Now, we can assign the elapsed time of the CPU1 by defining the start and the end quantities, i.e., <code>cpu1_start</code> and <code>cpu1_end</code>. To assign the units, we also need to import the Pint utility class <code>ureg</code>: <pre><code>from nomad.units import ureg\nsimulation.cpu1_start = 0 * ureg.second\nsimulation.cpu1_end = 30 * ureg.second + 24 * ureg.minute\n</code></pre></p> <p>In seconds, the elapsed time can be printed by doing: <pre><code>simulation.cpu1_end - simulation.cpu1_start\n</code></pre> which is 1470 seconds. In hours, we can use the method <code>to('hour')</code>: <pre><code>(simulation.cpu1_end - simulation.cpu1_start).to('hour')\n</code></pre> which give us approximately 0.4083 hours.</p>"},{"location":"nomad_simulations/#sub-sections-in-simulation","title":"Main sub-sections in <code>Simulation</code>","text":"<p>The <code>Simulation</code> section is composed of 4 main sub-sections:</p> <ol> <li><code>Program</code>: contains all the program metadata, e.g., <code>name</code> of the program, <code>version</code>, etc.</li> <li><code>ModelSystem</code>: contains all the system metadata about geometrical positions of atoms, their states, simulation cells, symmetry information, etc.</li> <li><code>ModelMethod</code>: contains all the methodological metadata, and is divided in two main aspects: the mathematical model or approximation used in the simulation (e.g., <code>DFT</code>, <code>GW</code>, <code>ForceFields</code>, etc.) and the numerical settings used to compute the properties (e.g., meshes, self-consistent parameters, basis sets settings, etc.).</li> <li><code>Outputs</code>: contains all the output properties obtained during the simulation.</li> </ol> <p>Self-consistent steps, SinglePoint entries, and more complex workflows.</p> <p>The minimal unit for storing data in the NOMAD archive is an entry. In the context of simulation data, an entry may contain data from a calculation on an individual system configuration (e.g., a single-point DFT calculation) using only the above-mentioned sections of the <code>Simulation</code> section. Information from self-consistent iterations to converge properties for this configuration are also contained within these sections.</p> <p>More complex calculations that involve multiple configurations require the definition of a workflow section within the archive. Depending on the situation, the information from individual workflow steps may be stored within a single or multiple entries. For example, for efficiency, the data from workflows involving a large amount of configurations, e.g., molecular dynamics trajectories, are stored within a single entry. Other standard workflows store the single-point data in separate entries, e.g.,  a <code>GW</code> calculation is composed of a <code>DFT SinglePoint</code> entry and a <code>GW SinglePoint</code> entry. Higher-level workflows, which simply connect a series of standard or custom workflows, are typically stored as a separate entry. See Part V - Custom Workflows for more information.</p> <p>The following schematic represents a simplified representation of the <code>Simulation</code> section (note that the arrows here are a simple way of visually defining inputs and outputs):</p>"},{"location":"nomad_simulations/#program","title":"<code>Program</code>","text":"<p>The <code>Program</code> section contains all the information about the program / software / code used to perform the simulation. The detailed relationship tree is:</p> <p>Note that the rhombo-headed arrow here indicates a composition / has a relationship, so that <code>BaseSimulation</code> has a <code>Program</code> sub-section under it.</p> <p>Assignment 2.2</p> <p>Instantiate a <code>Program</code> section and directly assign the name <code>'VASP'</code> and the version <code>'5.0.0'</code> quantities. Add this sub-section to the <code>Simulation</code> section created in the Assignment 2.1. Can you re-assign the <code>Program.version</code> quantity to be an integer number, 5?</p> Solution 2.2 <p>We can import and assign directly quantities of sections by doing: <pre><code>from nomad_simulations.schema_packages.general import Program\nprogram = Program(name='VASP', version='5.0.0')\n</code></pre></p> <p>And we can add it as a sub-section of <code>Simulation</code> by assigning the attribute of that class: <pre><code>simulation.program = program\n</code></pre></p> <p>If we try to re-assign: <pre><code>program.version = 5\n</code></pre> the code will complain with a <code>TypeError</code>: <pre><code>TypeError: The value 5 with type &lt;class 'int'&gt; for quantity nomad_simulations.schema_packages.general.Program.version:Quantity is not of type &lt;class 'str'&gt;\n</code></pre> This is because in the defintion of the <code>class Program</code> and the <code>Quantity:version</code>, we defined the type to be a string. So answering the question: no, it is not possible to re-assign <code>version</code> to be an integer due to the fact that is defined to be a string.</p>"},{"location":"nomad_simulations/#modelmethod","title":"<code>ModelMethod</code>","text":"<p>The <code>ModelMethod</code> section is an input section which contains all the information about the mathematical model used to perform the simulation. In NOMAD, we can extend the support of certain methods by inheriting from <code>ModelMethod</code> and extend the schema for the new methodology. <code>ModelMethod</code> also contains a specialized sub-section called <code>NumericalSettings</code>. The detailed relationship tree is:</p> <p><code>ModelMethod</code> is thus a sub-section under <code>Simulation</code>. It inherits from an abstract section <code>BaseModelMethod</code>, as well as containing a sub-section called <code>contributions</code> of the same section. The underlying idea of <code>ModelMethod</code> is to parse the input parameters of the mathematical model, typically a Hamiltonian. This total Hamiltonian or model could be split into individual sub-terms or <code>contributions</code>. Each of the electronic-structure methodologies inherit from <code>ModelMethodElectronic</code>, which contains a boolean <code>is_spin_polarized</code> indicating if the <code>Simulation</code> is spin polarized or not. The different levels of abstractions are useful when dealing with commonalities amongst the methods.</p> <p>Assignment 2.3</p> <p>Instantiate a <code>DFT</code> section. For simplicity, you can also assign the <code>jacobs_ladder</code> quantity to be <code>'LDA'</code>. Add this sub-section to the <code>Simulation</code> section created in the Assignment 2.1. What is the underlying concept that allows you to add directly the <code>class DFT</code> under <code>Simulation.model_method</code>, provided that the definition of this attribute is a <code>ModelMethod</code> sub-section? Can you reason why the current schema (version 0.0.2) is inconsistent in handling the <code>xc_functionals</code> contributions?</p> Solution 2.3 <p>Similarly to Assignment 2.2, we can import and create the <code>DFT</code> section: <pre><code>from nomad_simulations.schema_packages.model_method import DFT\ndft = DFT(jacobs_ladder='LDA')\n</code></pre></p> <p>This time, due to the fact that <code>Simulation.model_method</code> is a repeating sub-section (i.e., a list of sub-sections), we need to append <code>dft</code> instead of directly assigning the attribute: <pre><code>simulation.model_method.append(dft)\n</code></pre></p> <p>Thanks to inheritance of <code>class DFT</code> with <code>ModelMethod</code> and polymorphism, we can directly append <code>dft</code> as a <code>model_method</code> sub-section.</p> <p>The current schema is also a bit inconsistent due to the fact that <code>BaseModelMethod</code> has a sub-section called <code>contributions</code>, while <code>DFT</code> has also a sub-section called <code>xc_functionals</code>, hence both sub-sections live at the same time under the section <code>DFT</code>. Conceptually, both sub-sections are the same: they refer to a sub-term or contribution of the total DFT Hamiltonian, thus, their definitions should be combined, and only one sub-section should be used. The best action here would be to open an issue in the <code>nomad-simulations</code> Github repository, or directly contact the maintainers.</p>"},{"location":"nomad_simulations/#numericalsettings","title":"<code>NumericalSettings</code>","text":"<p>The <code>NumericalSettings</code> section is an abstract section used to define the numerical parameters set during the simulation, e.g., the plane-wave basis cutoff used, the k-mesh, etc. These parameters can be defined into specialized classes which inherit from <code>NumericalSettings</code> (similar to what happens with all the electronic-structure methodologies and <code>ModelMethod</code>). The detailed relationship tree is:</p> <p>Assignment 2.4</p> <p>Instantiate a <code>SelfConsistency</code> section and assign the quantity <code>threshold_change</code> to be <code>1e-3</code> and the <code>threshold_change_unit</code> to be <code>'joule'</code>. Add this sub-section to the <code>DFT</code> section created in the Assignment 2.3. Is the new information also stored in the <code>Simulation</code> section created in Assignment 2.1? Can you access the information of the Jacobs ladder string used in this simulation starting from the newly instantiated class?</p> Solution 2.4 <p>We can import and create the class <code>SelfConsistency</code>, assign the specified quantities, and append it under <code>dft</code>: <pre><code>from nomad_simulations.schema_packages.numerical_settings import SelfConsistency\nscf = SelfConsistency(threshold_change=1e-3, threshold_change_unit='joule')\ndft.numerical_settings.append(scf)\n</code></pre></p> <p>In order to see if <code>scf</code> is showing directly under <code>simulation</code>, we can: <pre><code>simulation.model_method[0].numerical_settings\n</code></pre> which indeed returns: <pre><code>[SelfConsistency:SelfConsistency(name, threshold_change, threshold_change_unit)]\n</code></pre></p> <p>In order to go from <code>scf</code> to the <code>dft.jacobs_ladder</code> information, we need to go one level up with respect to <code>scf</code>. In order to do this, we can use the method <code>m_parent</code>: <pre><code>scf.m_parent.jacobs_ladder\n</code></pre> which will return: <pre><code>'LDA'\n</code></pre></p>"},{"location":"nomad_simulations/#modelsystem","title":"<code>ModelSystem</code>","text":"<p>The <code>ModelSystem</code> section is an input section which contains all the information about the geometrical space quantities (positions, lattice vectors, volumes, etc) of the simulated system. This section contains various quantities and sub-sections which aim to describe the system in the most complete way and in a variety of cases, from unit cells of crystals and molecules up to microstructures. In order to handle this hierarchical structure, <code>ModelSystem</code> is nested over itself, i.e., a <code>ModelSystem</code> can be composed of sub-systems, which at the same time could be composed of smaller sub-systems, and so on. This is done thanks to the (proxy) sub-section attribute called <code>model_system</code>.</p> <p>The <code>Cell</code> sub-section is an important section which contains information of the simulated cell, including the <code>lattice_vectors</code> and the <code>positions</code> of the particles within. However, it does not contain specific information about these particles, e.g., their chemical identity or electronic state, as this is the responsability of a more specialized section, the <code>AtomicCell</code>. This section stores the relevant information about each of the atoms constituting the material via the <code>AtomsState</code> sub-section. The <code>Symmetry</code> sub-section contains standard symmetry classifications of the system, while the <code>ChemicalFormula</code> sub-section stores various strings that allow the system to be identified in a specific format of the chemical formulas (IUPAC, Hill, etc).</p> <p>The detailed relationship tree is:</p> <p>The <code>Entity</code> abstract section is defined in the Basic Formal Ontology (BFO) similar to <code>Activity</code>, and we use it to abstract our <code>ModelSystem</code>. In fact, <code>ModelSystem</code> is inheriting from an intermediate abstract section called <code>System</code>. This base section, <code>System</code>, is also used by the experimental data models to define the composition and structure of the measured materials.</p> GeometricSpace and simulated cells. <p>The abstract section <code>GeometricSpace</code> is used to define more general real space quantities related with the system of reference used, areas, lengths, volumes, etc. However, this section and <code>Cell</code> are currently (version 0.0.2) under revision and will probably change in the near future.</p>"},{"location":"nomad_simulations/#atomsstate","title":"<code>AtomsState</code> and other sub-sections","text":"<p>The <code>AtomsState</code> section is a list of sub-sections within <code>AtomicCell</code>, corresponding to the list of particles specified by the <code>positions</code> array defined under <code>Cell</code>. Each <code>AtomsState</code> section contains information about a specific chemical element used in the simulation, and may also contain additional <code>OrbitalsState</code>, <code>CoreHole</code>, or <code>HubbardInteractions</code> information. The detailed relationship tree is:</p> <p>Assignment 2.5</p> <p>Instantiate two <code>AtomsState</code> sections and assign the <code>chemical_symbol</code> to be <code>'Ga'</code> and <code>'As'</code> for each of these sections. For this assignment, we also need to define a <code>logger</code> object in order for the functionalities to work: <pre><code>from nomad import utils\nlogger = utils.get_logger(__name__)\n</code></pre> Using a method of <code>AtomsState</code>, what is the atomic number of Ga and of As?</p> Solution 2.5 <p>We can import and create two instances of <code>AtomsState</code> and assign <code>chemical_symbol</code> in a variety of ways. In this case, we used list comprehension: <pre><code>from nomad_simulations.schema_packages.atoms_state import AtomsState\natoms_states = [AtomsState(chemical_symbol=element) for element in ['Ga', 'As']]\n</code></pre></p> <p>For each of the atoms, we can use the class method <code>resolve_atomic_number(logger)</code> which will directly return the atomic number of each section: <pre><code>[atom.resolve_atomic_number(logger=logger) for atom in atoms_states]\n</code></pre> which returns: <pre><code>[31, 33]\n</code></pre> Hence, the atomic number of Ga is 31 and of As is 33.</p> <p>Assignment 2.6</p> <p>Instantiate a <code>ModelSystem</code> section and a <code>AtomicCell</code> section. Assign the <code>positions</code> in the <code>AtomicCell</code> section to be <code>[[0, 0, 0], [1, 1, 1]]</code> in meters. Add the <code>atoms_states</code> defined in the Assignment 2.5 under the <code>AtomicCell</code> section. Append this <code>AtomicCell</code> section under <code>ModelSystem</code>, and append <code>ModelSystem</code> to the <code>Simulation</code> section created in Assignment 2.1.</p> <p>Now, we want to extract the different formats in which the chemical formulas of this system can be written. For that, instantiate directly the <code>ChemicalFormula</code> sub-section under <code>ModelSystem</code>. Note: you can use the <code>ChemicalFormula.normalize(archive, logger)</code> method, and pass <code>archive=None</code> to this function.</p> <p>What are the different formats of the chemical formula? What is the string of the <code>descriptive</code> format and why it coincides with other format(s) in the sub-section?</p> Solution 2.6 <p>We can import and create instances of <code>ModelSystem</code> and <code>AtomicCell</code> and assign the corresponding quantities directly: <pre><code>from nomad_simulations.schema_packages.model_system import ModelSystem, AtomicCell\nmodel_system = ModelSystem()\natomic_cell = AtomicCell(atoms_state=atoms_states, positions=[[0, 0, 0], [1, 1, 1]] * ureg.meter)\nmodel_system.cell.append(atomic_cell)\n</code></pre></p> <p>We can append this section to <code>Simulation</code>: <pre><code>simulation.model_system.append(model_system)\n</code></pre></p> <p>We can now assign directly an empty <code>ChemicalFormula</code> section to <code>ModelSystem</code>, in order to be able to call for the specific class method: <pre><code>from nomad_simulations.schema_packages.model_system import ChemicalFormula\nmodel_system.chemical_formula = ChemicalFormula()\n</code></pre></p> <p>In order to extract the formulas and assign them to the quantities of the section <code>ChemicalFormula</code>, we can use the method <code>normalize()</code>. This class method takes two arguments as inputs: <code>archive</code> and <code>logger</code>. For the purpose of this tutorial, we can set <code>archive=None</code>: <pre><code>model_system.chemical_formula.normalize(archive=None, logger=logger)\n</code></pre></p> <p>Now, the <code>ChemicalFormula</code> sub-section contains the information of the different formats for the chemical formula: <code>reduced</code> and <code>hill</code> are both <code>'AsGa'</code>, while <code>iupac</code> is <code>'GaAs'</code>. The <code>descriptive</code> formula is set depending on the chemical elements present in the system, and because neither <code>'H'</code> nor <code>'O'</code> is present (i.e., no organic formulas), and gallium arsenide does not belong to any format exceptions, then it is set to the inorganic typical formula <code>'GaAs'</code>, coinciding with <code>iupac</code>. The <code>anonymous</code> formula is <code>'AB'</code>.</p>"},{"location":"nomad_simulations/#outputs","title":"<code>Outputs</code>","text":"<p>The <code>Outputs</code> section contains all the information about the properties computed by the simulations, as well as references to the relevant <code>ModelMethod</code> and <code>ModelSystem</code>. Each property is stored individually under <code>Outputs</code>, and inherits from an abstract section call <code>PhysicalProperty</code>. The <code>PhysicalProperty</code> base section contains the <code>value</code> of the property along with other relevant quantities. The <code>variables</code> sub-section enables the physical property to be stored as a function of a varying parameter. Accordingly, the shape of value is calculated in a dynamic way. This means that the same physical property (e.g., <code>ElectronicBandGap</code>) could have different shapes depending on the use-case being parsed (e.g., a single scalar number or a set of varying scalars with respect to a variable, e.g., <code>Temperature</code>).</p> <p>The <code>Outputs</code> section can be further specialized into <code>SCFOutputs</code> in case the properties are calculated in a series of self-consistent steps. The steps are stored under a sub-section called <code>scf_steps</code>, while the last step and other non-self-consistently calculated properties are stored directly under <code>SCFOutputs</code>. The reference to the <code>SelfConsistency</code> section (see NumericalSettings) allows us to automatically determine if a self-consistently calculated property is converged or not.</p> <p>The detailed relationship tree is:</p> <p>Assignment 2.7</p> <p>Instantiate an <code>SCFOutputs</code> section. We are going to store a self-consistently calculated <code>FermiLevel</code>, whose values at each step are <code>[1, 1.5, 2, 2.1, 2.101]</code> in eV. Add the appropiate references to the <code>ModelMethod</code> and <code>ModelSystem</code> sections created in Assignment 2.3 and Assignment 2.6, respectively. For the self-consistently calculated <code>FermiLevel</code> section, add the reference to the section <code>SelfConsistency</code> created in Assignment 2.4.</p> <p>Check if the <code>FermiLevel</code> is self-consistently converged or not by using a class method from <code>SCFOutputs</code>. What happens if the <code>SelfConsistency.threshold_change</code> is now <code>1e-24</code>?</p> Solution 2.7 <p>We can import and create an instance of <code>SCFOutputs</code> and append it to simulation: <pre><code>from nomad_simulations.schema_packages.outputs import SCFOutputs\nscf_outputs = SCFOutputs()\nsimulation.outputs.append(scf_outputs)\n</code></pre></p> <p>We can add the references to the other <code>ModelMethod</code> and <code>ModelSystem</code> sections by doing: <pre><code>scf_outputs.model_method_ref = simulation.model_method[0]\nscf_outputs.model_system_ref = simulation.model_system[0]\n</code></pre></p> <p>Now, we need to create the <code>scf_steps</code> sub-sections with the information of the <code>FermiLevel</code> steps and their values. For that, <pre><code>from nomad_simulations.schema_packages.outputs import Outputs\nfrom nomad_simulations.schema_packages.properties import FermiLevel\nfor value in [1, 1.5, 2, 2.1, 2.101]:\n    fermi_level = FermiLevel()\n    fermi_level.value = value * ureg.eV\n    scf_outputs.scf_steps.append(Outputs(fermi_levels=[fermi_level]))\n</code></pre> Note that:</p> <ol> <li>The properties like <code>fermi_levels</code> are repeated sub-sections under <code>Outputs</code>, so we need to assign a list.</li> <li>The <code>scf_steps</code> sub-sections are <code>Outputs</code>, hence we need to import the <code>Outputs</code> section and append it to that attribute.</li> </ol> <p>We also need to add the last step directly under <code>scf_outputs</code> and add the reference to the <code>SelfConsistency</code> section: <pre><code>scf_outputs.fermi_levels.append(FermiLevel(value=2.101 * ureg.eV, self_consistency_ref=simulation.model_method[0].numerical_settings[0]))\n</code></pre></p> <p>In order to check if the <code>FermiLevel</code> is converged or not, we can use the class method <code>resolve_is_scf_converged()</code>. This method has various inputs that are explained in the method itself. Here we will simply use the function like: <pre><code>scf_outputs.resolve_is_scf_converged(property_name='fermi_levels', i_property=0, physical_property=scf_outputs.fermi_levels[0], logger=logger)\n</code></pre> which indeed returns <pre><code>True\n</code></pre> So the <code>FermiLevel</code> is converged.</p> <p>Now, if we set: <pre><code>scf.threshold_change = 1e-24\n</code></pre> And re-run the <code>resolve_is_scf_converged()</code> method, we can see that the <code>FermiLevel</code> is not self-consistenly converged: <pre><code>False\n</code></pre></p> <p>The reason for this is that, in joules, the last two self-consistent steps (where the Fermi level is 2.1 and 2.101 eV) have a difference which can be also computed with another class method: <pre><code>scf_values = scf_outputs.get_last_scf_steps_value(scf_last_steps=scf_outputs.scf_steps[-2:], property_name='fermi_levels', i_property=0, scf_parameters=scf, logger=logger)\nabs(scf_values[0] - scf_values[1])\n</code></pre> which returns: <pre><code>1.6021766340002688e-22\n</code></pre> Then, a <code>threshold_change</code> of <code>1e-24</code> is smaller than the difference, so that the <code>FermiLevel</code> is not converged.</p> <p>Assignment 2.8</p> <p>We are going to store two <code>ElectronicBandGap</code> property sections under the <code>SCFOutputs</code> section created in the Assignment 2.7:</p> <ul> <li>An <code>ElectronicBandGap</code> whose value is 2 eV.</li> <li>An <code>ElectronicBandGap</code> varying with <code>Temperature</code>, whose values are <code>[1, 1.5, 2]</code> in eV for <code>[100, 150, 200]</code> temperatures in Kelvin.</li> </ul> <p>In the second situation of an electronic band gap which varies with the <code>Temperature</code>, what happens if you directly assign <code>value</code> before defining the <code>Temperature</code> variables sub-section in <code>ElectronicBandGap</code>?</p> Solution 2.8 <p>We can import the <code>ElectronicBandGap</code> property and assign the first case of an electronic band gap which is 2 eV: <pre><code>from nomad_simulations.schema_packages.properties import ElectronicBandGap\nband_gap = ElectronicBandGap()\nband_gap.value = 2 * ureg.eV\nscf_outputs.electronic_band_gaps.append(band_gap)\n</code></pre></p> <p>We can do the same for the temperature-dependent band gap by importing and defining the <code>Temperature</code> variable sub-section (note that <code>variables</code> is a repeated sub-section, thus we need to assign a list): <pre><code>from nomad_simulations.schema_packages.variables import Temperature\nband_gap_T = ElectronicBandGap(variables=[Temperature(points=[100, 150, 200] * ureg.kelvin)])\nband_gap_T.value = [1, 1.5, 2] * ureg.eV\nscf_outputs.electronic_band_gaps.append(band_gap_T)\n</code></pre></p> <p>Here, we have assigned first <code>variables</code> before the <code>value</code> of the property. However, if we want to assign first the <code>value</code>: <pre><code>band_gap_T = ElectronicBandGap()\nband_gap_T.value = [1, 1.5, 2] * ureg.eV\n</code></pre> we will get a <code>ValueError</code> due to the fact that <code>variables</code> is not set and the shape of <code>value</code> is not empty: <pre><code>ValueError: The shape of the stored `value` [3] does not match the full shape [] extracted from the variables `n_points` and the `shape` defined in `PhysicalProperty`.\n</code></pre></p> <p>Thus, the <code>variables</code> sub-sections must be set before setting the <code>value</code> of a physical property. This is because the <code>class PhysicalProperty</code> is doing validations on the shape of the <code>variables</code> and <code>value</code>, and it only works if <code>variables</code> is set first. Only when the shape of <code>value</code> is empty and due to the fact that <code>variables</code> is set by default to an empty list, then we would not get any error.</p>"},{"location":"nomad_simulations/#normalize-function","title":"Extra: The <code>normalize()</code> class function","text":"<p>Each base section defined using the NOMAD schema has a set of public functions which can be used at any moment when reading and parsing files in NOMAD. The <code>normalize(archive, logger)</code> function is a special case of such functions, which warrants an in-depth description.</p> <p>This function is run within the NOMAD infrastructure by the <code>MetainfoNormalizer</code> in the following order:</p> <ol> <li>A child section's <code>normalize()</code> function is run before its parents' <code>normalize()</code> function.</li> <li>For sibling sections, the <code>normalize()</code> function is executed from the smaller to the larger <code>normalizer_level</code> attribute. If <code>normalizer_level</code> is not set or if they are the same for two different sections, the order is established by the attributes definition order in the parent section.</li> <li>Using <code>super().normalize(archive, logger)</code> runs the inherited section normalize function.</li> </ol> <p>Let's see some examples. Imagine having the following <code>Section</code> and <code>SubSection</code> structure:</p> <pre><code>from nomad.datamodel.data import ArchiveSection\n\n\nclass Section1(ArchiveSection):\n    normalizer_level = 1\n\n    def normalize(self, achive, logger):\n        # some operations here\n        pass\n\n\nclass Section2(ArchiveSection):\n    normalizer_level = 0\n\n    def normalize(self, achive, logger):\n        super().normalize(archive, logger)\n        # Some operations here or before `super().normalize(archive, logger)`\n\n\nclass ParentSection(ArchiveSection):\n\n    sub_section_1 = SubSection(Section1.m_def, repeats=False)\n\n    sub_section_2 = SubSection(Section2.m_def, repeats=True)\n\n    def normalize(self, achive, logger):\n        super().normalize(archive, logger)\n        # Some operations here or before `super().normalize(archive, logger)`\n</code></pre> <p>Now, <code>MetainfoNormalizer</code> will be run on the <code>ParentSection</code>. Applying rule 1, the <code>normalize()</code> functions of the <code>ParentSection</code>'s childs are executed first. The order of these functions is established by rule 2 with the <code>normalizer_level</code> atrribute, i.e., all the <code>Section2</code> (note that <code>sub_section_2</code> is a list of sections) <code>normalize()</code> functions are run first, then <code>Section1.normalize()</code>. Then, the order of execution will be:</p> <ol> <li><code>Section2.normalize()</code></li> <li><code>Section1.normalize()</code></li> <li><code>ParentSection.normalize()</code></li> </ol> <p>In case we do not assign a value to <code>Section1.normalizer_level</code> and <code>Section2.normalizer_level</code>, <code>Section1.normalize()</code> will run first before <code>Section2.normalize()</code>, due to the order of <code>SubSection</code> attributes in <code>ParentSection</code>. Thus the order will be in this case:</p> <ol> <li><code>Section1.normalize()</code></li> <li><code>Section2.normalize()</code></li> <li><code>ParentSection.normalize()</code></li> </ol> <p>By checking on the <code>normalize()</code> functions and rule 3, we can establish whether <code>ArchiveSection.normalize()</code> will be run or not. In <code>Section1.normalize()</code>, it will not, while in the other sections, <code>Section2</code> and <code>ParentSection</code>, it will.</p>"},{"location":"parser_plugins/","title":"Part 3 - Creating Parser Plugins from Scratch","text":"<p>One of NOMAD's most recognized features is drag-and-drop parsing. The NOMAD parsers, which automate the conversion of raw simulation files into the standardized NOMAD format, significantly offload the burden of data annotation from researchers, reducing their data management responsibilities while also improving the accessibility of their data.</p> <p>Behind the scenes, parsing beings with looking through the upload folder and selecting relevant files. These files are then read in and their data extracted. Lastly, the semantics of the file format are clarified and specified as its data is mapped into the NOMAD schema. The data is now ready to interact with the NOMAD ecosystem and apps.</p>"},{"location":"parser_plugins/#fundamentals-of-parsing","title":"Fundamentals of Parsing","text":"<p>As parsing involves the mapping between two well-defined formats, one could expect it to be trivial. In practice, however, parser developers have to manage discrepancies in semantics, shape, data type or units. This has lead to five distinct categories of responsibility for the developer to manage. Here, they are ordered to match the parser's execution:</p> <ol> <li>file selection - navigate the upload's folder structure and select the relevant files.</li> <li>source extraction - read the files into Python. This step may already include some level of data field filtering.</li> <li>source to target - map the data of interest with their counterparts in the target/NOMAD schema. This is where the bulk of the filtering happens.</li> <li>data mangling - manipulate the data to match the target/NOMAD <code>Quantity</code>s' specification, e.g. dimensionality, shape. This may include computing derived properties not present in the original source files.</li> <li>archive construction - build up a Python <code>EntryArchive</code> object using the classes provided by the target/NOMAD schema. NOMAD will automatically write and commit it to the database as an <code>archive.json</code></li> </ol> <p>Blurring these responsibilities leads to a wild-growth in parser design and added complexity, especially in larger, more feature-rich parsers. NOMAD therefore offers powerful tools and documentation on best practices to help the parser developer manage each distinct responsibility. The exact solutions are, in the same order:</p> <ol> <li><code>MatchingParser</code> - This class selects the file to be parsed. Since it interfaces with the NOMAD base directly, it will read in most of the settings automatically from there.</li> <li><code>XMLParser</code> and co. / <code>TextParser</code> - There are several reader classes for loading common source formats into Python data types. Examples include the <code>XMLParser</code> and <code>HDF5Parser</code>. We will demonstrate <code>XMLParser</code> in Parsing Hierarchical Tree Formats. Plain text files, meanwhile, involve an additional matching step via the <code>TextParser</code>. More on this in From Text to Hierarchies.</li> <li><code>MappingAnnotationModel</code> - This is arguably the most involved part for the parser developer, as this is where the external data gets further semantically enriched and standardized. It requires domain expertise to understand the relationship between the data fields in the source file and the NOMAD-Simulations schema. If step 2 went well, step 3 could just involve annotating the target/NOMAD schema. We show how this is conceptually possible with <code>MappingAnnotationModel</code> in Mapping a to Schema, but note that it still at the prototype stage.</li> <li>NOMAD-Simulations schema / <code>MappingAnnotationModel</code> - The <code>MSection</code>s and <code>utils.py</code> in the schema provide normalizers and helper functions to alleviate most of the data mangling. For small amendments, use the mapping approach described in Via Mapping. For larger ones, consider extending the schema as covered in Extending NOMAD-Simulations.</li> <li><code>MetainfoParser</code> - this converter bridges the annotated schema from step 3 with the reader classes in step 2. <code>MetainfoParser.data_object</code> contains the final <code>ArchiveSection</code> that is stored under <code>archive.data</code>.</li> </ol> <p>In the next section, we will briefly illustrate how <code>MatchingParser</code>, <code>XMLParser</code>, and <code>MetainfoParser</code> interconnect, as well as flesh out some setup details.</p>"},{"location":"parser_plugins/#starting-a-plugin-project","title":"Starting a Plugin Project","text":"<p>To create your own parser plugin, visit our plugin template and click the \u201cUse this template\u201d button. Follow the How to get started with plugins documentation for detailed setup instructions. The template will appear bare-bones at the start. Following the instructions in the <code>README.md</code>  and the <code>cruft</code> setup will allow you to tune the project to your needs.</p> <p>By the end, your repository should look like the example below. Make sure to include both a <code>parsers</code> and a <code>schema_packages</code> folder. Each file parser under <code>parsers</code> gets its own <code>.py</code> file. For schemas, one file typically suffices, unless you want to heavily extend them (see Extending NOMAD-Simulations). Note that <code>myparser.py</code> and <code>mypackage.py</code> just act as a blueprints. They should not be edited or exposed directly.</p> <p>The <code>README.md</code> should remain a copy of the template, in case anyone wants to fork the project. Instead, place plugin descriptions in the entry points or GitHub repository metadata. More elaborate explanations should go under <code>docs</code>. You can deploy them on GitHub or locally via <code>mkdocs</code>. Lastly, NOMAD uses the Apache 2.0 license (option 4 in the <code>cruft</code> setup). Please select the same license for maximal legal compatibility.</p> <pre><code>.\n\u2514\u2500\u2500 nomad-plugin-parser/\n    \u251c\u2500\u2500 src/\n    \u2502   \u2514\u2500\u2500 nomad_plugin_parser/\n    \u2502       \u251c\u2500\u2500 __init__.py\n    \u2502       \u251c\u2500\u2500 parsers/\n    \u2502       \u2502   \u251c\u2500\u2500 __init__.py     (entry point)\n    \u2502       \u2502   \u2514\u2500\u2500 myparser.py     (target functionality)\n    \u2502       \u2514\u2500\u2500 schema_packages/\n    \u2502           \u251c\u2500\u2500 __init__.py\n    \u2502           \u2514\u2500\u2500 mypackage.py    (extending the schema)\n    \u251c\u2500\u2500 docs/\n    \u251c\u2500\u2500 tests/\n    \u251c\u2500\u2500 pyproject.toml              (publication specifications)\n    \u251c\u2500\u2500 README.md\n    \u2514\u2500\u2500 LICENSE.txt\n\n---------------------------------------------------------------\n\u251c\u2500\u2500 nomad.yaml                      (NOMAD configurations)\n</code></pre> <p>This examples also highlights the files containing entry point information between parentheses. Entry points are the official mechanism for <code>pip</code> installing modules. Just note that their referencing runs bottom up, i.e. from the <code>publication specifications</code> to the <code>entry point</code> itself, and from thereon to the <code>target functionality</code>.</p> <p>Once the plugin is ready, you can add it to your local NOMAD deployment. We recommend you to read the details in the NOMAD documentation page regarding installing plugins in NOMAD Oasis. The <code>NOMAD configurations</code> refers back to the <code>publication specifications</code> to load plugins in your local NOMAD installation. It allows you to control the loading of plugins and their options, however, normally you don't have to touch anything there for NOMAD to pick up on your parser.</p> Managing Entry Points"},{"location":"parser_plugins/#what-are-they","title":"What are they?","text":"<p>The plugin setup follows the common entry-points Python standard from <code>importlib.metadata</code>. It works in tandem with <code>pip</code> install to allow for a more elegant and controlled way of exposing and loading (specific functionalities in) modules. Entry points also provide the module developer tools for controlling how it ought to be exposed to the environment, e.g. name, description, configuration.</p>"},{"location":"parser_plugins/#relation-with-the-python-environment","title":"Relation with the Python Environment","text":"<p>Contrary to regular dependencies, entry points are visible at a system-wide level, even when installed in a local environment. You can therefore choose whether to install plugins in their own environment, or construct a shared one (with the NOMAD base). We recommend the former to prevent dependency clashing.</p>"},{"location":"parser_plugins/#key-players","title":"Key Players","text":"<p>Conceptually, there are five roles to keep track off:</p> <ul> <li>target functionality: the entity that we want to expose to the NOMAD base installation. In our case, this will amount to our parser class, which typically is a child class of <code>MatchingParser</code>. It may use <code>configurations</code> parameters passed along via the nomad settings.</li> <li>entry point: instance of <code>EntryPoint</code>, responsible for registering the target functionality. It therefore also retains metadata like its name, a description and most importantly, the file matching directives. It is typically located in the <code>__init__.py</code> of the relevant functionality folder (see folder structure).<ul> <li>entry point group: bundles several entry points together. By default, NOMAD scans all plugins under the group name <code>project.entry-points.'nomad.plugin'</code>.</li> </ul> </li> <li>publication specifications: exposes the entry point (and its group) under the format of <code>&lt;module_name&gt;.&lt;object_name&gt;:&lt;entry_point_name&gt;</code>. This is the name by which you should refer to it within the entry point system. For importing the within a Python script, use only <code>&lt;module_name&gt;.&lt;object_name&gt;</code>. In NOMAD we use the <code>pyproject.toml</code> setup file under the module's root folder.</li> <li>NOMAD configurations: called in <code>nomad.yaml</code>, controls which entry points are included or excluded, as well as their configuration parameters.</li> </ul>"},{"location":"parser_plugins/#assembling-a-parser-class","title":"Assembling a Parser Class","text":"<p>Throughout this subsection, we will provide a step-by-step guide of the process for building out a parser using the VASP XML output format as an example. For an overview of the code in its complete form, you can check out the official repository. Make sure to fork it in case you want to track your modifications. The code snippets provided below should are located under <code>src/nomad_parser_vasp/parsers/</code>, in a file clarifying the intended extension, <code>xml_parser.py</code>.</p>"},{"location":"parser_plugins/#hooking-up-a-parser","title":"Hooking up a Parser","text":"<p>As denoted in step 1, the parser first has to read in the file contents as passed through by the NOMAD base. The directives for selecting mainfiles are passed on via an interaction cascade from <code>nomad.yaml</code> &gt; <code>ParserEntryPoint</code> &gt; <code>MatchingParser</code>.</p> What are mainfiles? <p>Mainfiles are files by which an expert / program can determine the code used / the parser to use. The selection directives (see below) target these files specifically. Their file paths are passed on to the parser, which can either process them or navigate the folder for other, auxiliary files.</p>"},{"location":"parser_plugins/#mainfile-matching","title":"Mainfile Matching","text":"<p>Since mainfiles are intrinsically connected to the scripts that parse them, the directives should be set via <code>ParserEntryPoint</code>. In rare cases, however, an Oasis admin may decide to override these selection directives via the <code>nomad.yaml</code>. We publish our VASP XML parser in <code>src/nomad_parser_vasp/parsers/__init__.py</code> as</p> <pre><code>from nomad.config.models.plugins import ParserEntryPoint\n\nclass VasprunXMLEntryPoint(ParserEntryPoint):\n    def load(self):\n        from nomad_parser_vasp.parsers.xml_parser import VasprunXMLParser\n        return VasprunXMLParser(**self.dict())\n\nxml_entry_point = VasprunXMLEntryPoint(\n    name='VasprunXML Parser',\n    description='Parser for VASP output in XML format.',\n    mainfile_name_re='.*vasprun\\.xml.*',\n)\n</code></pre> <p><code>load</code> comes from the entry point system and should just return our parser (see below). The entry point itself specifies parser data and the directives. There are three kinds of file aspects that can be targeted, all via regular expressions (regex):</p> <ul> <li><code>mainfile_name_re</code> - the filename itself.</li> <li><code>mainfile_contents_re</code>, <code>mainfile_contents_dict</code> - the file contents. It is by default restricted to the first 1024 bytes, i.e. the file header.</li> <li><code>mainfile_mime_re</code> - the file mime.  </li> </ul> <p>Assignment 3.1</p> <p>XML is a common file extension, and the user may remove <code>vasprun</code> from the name. Swap out <code>mainfile_name_re</code> for a different selection directive.</p> Solution 3.1 <p>VASP XML typically starts with the tags <pre><code>&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;\n&lt;modeling&gt;\n</code></pre> You can capture this via <code>mainfile_contents_re</code> in a regex like <pre><code>r'&lt;\\?xml version=\"1\\.0\" encoding=\"ISO\\-8859\\-1\"?&gt;\\n&lt;modeling&gt;\n</code></pre></p>"},{"location":"parser_plugins/#mainfile-interfacing","title":"Mainfile Interfacing","text":"<p>Within the cascade, <code>MatchingParser</code>, acts as the connection point on the parser side. It plays less of a role in manipulating the directives, and more so in defining the interface \u2014a formalization of mutually agreed upon behavior\u2014 back to the parser. The two main specifications are instantiation and <code>parse</code>. Since <code>MatchingParser</code> already defines both, parsers may simply inherit therefrom. The most rudimentary parser, thus looks as follows:</p> <pre><code>from nomad.parsing import MatchingParser\nclass VasprunXMLParser(MatchingParser):\n    pass\n</code></pre> Running your Parser <p>NOMAD can already run this parser, but will raise a <code>NotImplementedError</code>. The interface may be defined, but we still need to fill in the actual parsing by overwriting the default <code>parse(...)</code> function. That is for the next section.</p>"},{"location":"parser_plugins/#front-end","title":"Front-end","text":"<p>In everyday NOMAD use, the user only interacts with NOMAD via the GUI or API. NOMAD will regulate parsing as the user uploads via these channels.</p>"},{"location":"parser_plugins/#command-line","title":"Command-line","text":"<p>During development, the command line is probably the preferable option, as you can load changes faster and incorporate it into your favorite test setups. To print the archive to the terminal, use <code>nomad parse --show-archive &lt;mainfile&gt;</code>. If you already know which parser to use, add the <code>--parser 'parser/&lt;parser or entry point name&gt;'</code> flag. To list all options, type <code>nomad parse --help</code>. Note that even the command line passes through the NOMAD base, so make sure to have it installed and set up correctly.</p>"},{"location":"parser_plugins/#notebooks","title":"Notebooks","text":"<p>If you are using Jupyter Notebook, you can manipulate data in a head-on way without NOMAD base as an intermediary. Note that this enntails providing the parsing input yourself, as well as manually triggering normalization. A template setup looks something like:</p> <pre><code>from nomad.datamodel import EntryArchive\nfrom nomad.client.processing import parse\nfrom nomad.client import normalize_all\nfrom nomad.normalizing.metainfo import MetainfoNormalizer\nfrom &lt;parser_plugin&gt;.parser import &lt;ParserPlugin&gt;\n\np = ParserPlugin()\na = EntryArchive()\n\n# parsing ONLY\np.parse(&lt;mainfile&gt;, a, logger=None)\n\n# parsing + full normalization\na = parse(&lt;mainfile&gt;)\nnormalize_all(a[0])\n\n# parsing + schema-only normalization\np.parse(&lt;mainfile&gt;, a, logger=None)\nMetainfoNormalizer().normalize(archive=a)\n</code></pre>"},{"location":"parser_plugins/#getting-the-data","title":"Getting the Data","text":"<p>Where <code>MatchingParser</code> provides a path to the mainfiles, a separate parser is needed for actually reading the file contents. NOMAD already provides several parsers for popular, general-purpose formats like JSON, HDF5, and XML.  Plain text is also supported, but a bit more involved. We cover it in section From Text to Hierarchies.</p> <p>Contrary to <code>MatchingParser</code>, none of these parsers interface directly with the NOMAD base. For example, they do not support the <code>parse(...)</code> function. Therefore, our parser has to call <code>XMLParser</code>. The typical strategy here is to save the reader object for later manipulation. In the example below, we read in the whole file. The XPath syntax also supports subbranch extraction, which can be incrementally added to the reader object.</p> <pre><code>from structlog.stdlib import BoundLogger\nfrom nomad.datamodel.datamodel import EntryArchive\nfrom nomad.parsing.file_parser.xml_parser import XMLParser\n\nclass VasprunXMLParser(MatchingParser):\n    def parse(\n        self, mainfile: str, archive: EntryArchive, logger: BoundLogger,\n        child_archives: dict[str, EntryArchive] = None,\n    ) -&gt; None:\n        xml_reader = XMLParser(mainfile=mainfile).parse('/*')  # XPath syntax\n        archive.data = xml_reader._results\n</code></pre> To Return or not to Return <p>Should <code>parse(...)</code> return a filled out <code>EntryArchive</code> object to the NOMAD base or rather overwrite <code>archive</code>? Its type signature, i.e. <code>-&gt; None</code>, denotes that it should in fact not return any output.</p> <p>In NOMAD, we use type signatures as much as possible. They are also tested in our CI/CD, which might request adding them in cases where types cannot be inferred.</p> <p>The <code>NotImplementedError</code> is now resolved. Running the parser results in a new error, however, <code>AttributeError: 'dict' object has no attribute 'm_parent'</code>. This may look discouraging, but progress was made! The data has been successfully extracted: check <code>xml_reader._results</code>. The issue stems from data not yet meeting the high-quality standards of the NOMAD schema. In the next section, we convert it.</p> What is the Archive? <p>An archive is a (typically empty) storage object for an entry. It is populated by the parser and later on serialized into an <code>archive.json</code> file by NOMAD for permanent storage.</p> <p>It has five sections, but for novel parsers we are solely interested in <code>data</code> and <code>workflow</code>.</p> <ul> <li><code>metainfo</code>: internal NOMAD metadata registering who uploaded the data and when is was uploaded. This is handled completely automatically by the NOMAD base.</li> <li><code>results</code>: the data indexed and ready to query at full database scale. It is automatically produced from <code>worfklow</code>, <code>data</code>, and <code>run</code>.</li> <li><code>workflow</code>: some entries coordinate other entries. This section coordinates the . For more, see Interfacing complex simulations.</li> <li><code>data</code>: the new section detailing all extracted values. It comes with the updated schema presented in NOMAD-Simulations schema plugin.</li> <li><code>run</code>: the predecessor to <code>data</code>. It should only be targeted by legacy parsers, and has been marked for deprecation.</li> </ul> Communicating via Logs <p>Each entry has an associated log.  These communicate info or warnings about the processing, as well as debugging info and (critical) errors for tracing bugs. Include the latter when contacting us regarding any processing issues.</p> <p>Here, we use the <code>logger</code> object to <code>info</code>rm which parser was called.</p> <pre><code>...\nfrom nomad.config import config\n\nconfiguration = config.get_plugin_entry_point(\n    'nomad_parser_vasp.parsers:xml_entry_point'\n)\n\nclass VasprunXMLParser(MatchingParser):\n    def parse(\n        self, mainfile: str, archive: EntryArchive, logger: BoundLogger,\n        child_archives: dict[str, EntryArchive] = None,\n    ) -&gt; None:\n\n    logger.info('VasprunXMLParser.parse', parameter=configuration.parameter)\n    ...\n</code></pre>"},{"location":"parser_plugins/#from-parser-to-nomad","title":"From Parser to NOMAD","text":"<p>We conceptualize format conversion as restructuring a hierarchical data tree, formally known as a directed acyclic graph. A tree structure always starts at a common node, the root, where it splits of into several branches. Each node is a new potential splitting point. When a branch ends and no further splitting occurs, we call the node a leaf.</p> <p>The restructuring may then be defined in terms of lining up source with target nodes. The only missing component then are the mappings themselves.</p>"},{"location":"parser_plugins/#via-instantiation","title":"Via Instantiation","text":"<p>There are two ways of populating the NOMAD schema. The most basic one, on which you can always fall back, consists in reconstructing it from the root down to the leaf nodes. Given that the new NOMAD Simulations schema is quite shallow, this becomes more so a task in retrieving the section that best matches the semantics. The tree representation in the example below could be understood as</p> <pre><code>      Simulation\n      /       \\\n Program      ModelSystem\n  /   \\           |\nname version  AtomicCell\n                  |\n               positions\n</code></pre> <p>In the parser code itself, the sections and quantities are both just classes. We thus construct a tree of their objects by instantiating them one-by-one.</p> <pre><code>from nomad.units import ureg\n...\n\nclass VasprunXMLParser(MatchingParser):\n    def parse(\n        self,\n        mainfile: str,\n        archive: EntryArchive,\n        logger: BoundLogger,\n        child_archives: dict[str, EntryArchive] = None,\n    ) -&gt; None:\n        xml_reader = XMLParser(mainfile=mainfile)\n\n        def xml_get(path: str):\n            return xml_reader.parse(path)._results[path]\n\n        archive.data = Simulation(\n            program=Program(\n                name='VASP',\n                version=xml_get(\"//generator/i[@name='version']\")[0],\n            ),\n            model_system=ModelSystem(\n                cell=AtomicCell(\n                    positions=np.float64(xml_get(\"structure[@name='finalpos']/./varray[@name='positions']/v\")[0]) * ureg.angstrom,\n                ),\n            ),\n        )\n</code></pre> Data Type Conversion <p>NOMAD strictly enforces <code>Quantity</code> types like <code>np.int32</code>, <code>np.float64</code>, <code>np.complex128</code>, <code>str</code>, etc. These do no necessarily include all concepts from the <code>typing</code> or <code>numpy</code> module. <code>List</code>, for example, is controlled via the <code>Quantity.shape</code> attribute.</p> <p>Note that you should apply the proper type before storing the quantity value, as conversions that lead to loss of precision are returned as errors, e.g. <code>int</code> cannot be readily cast into <code>np.float64</code>.</p> <p>The final <code>archive.json</code> will look something like the following. Obviously, the exact values depend on the file parsed.</p> <pre><code>{\n  \"data\": {\n    \"m_def\": \"nomad_simulations.general.Simulation\",\n    \"program\": {\n      \"name\": \"VASP\",\n      \"version\": \"5.3.2\"\n    },\n    \"model_system\": [\n      {\n        \"datetime\": \"2024-07-08T13:25:23.509517+00:00\",\n        \"branch_depth\": 0,\n        \"cell\": [\n          {\n            \"m_def\": \"nomad_simulations.model_system.AtomicCell\",\n            \"name\": \"AtomicCell\",\n            \"positions\": [\n              [\n                0.0,\n                0.0,\n                0.0\n              ],\n              [\n                0.500001,\n                0.500001,\n                0.500001\n              ]\n            ]\n          }\n        ]\n      }\n    ]\n  },\n  \"metadata\": {\n    \"entry_name\": \"vasprun.xml.relax\",\n    \"mainfile\": \"/home/nathan/Downloads/vasprun.xml.relax\",\n    \"text_search_contents\": [\n      \"VASP\",\n      \"[]\",\n      \"AtomicCell\"\n    ],\n    \"domain\": \"dft\",\n    \"n_quantities\": 21,\n    \"quantities\": [\n      ...,\n      \"results.properties\"\n    ],\n    \"sections\": [\n      ...,\n      \"nomad_simulations.model_system.ModelSystem\"\n    ],\n    \"section_defs\": [\n      ...,\n      {\n        \"id\": \"data.model_system.branch_depth#nomad_simulations.general.Simulation\",\n        \"definition\": \"nomad_simulations.model_system.ModelSystem.branch_depth\",\n        \"path_archive\": \"data.model_system.0.branch_depth\",\n        \"int_value\": 0\n      }\n    ]\n  },\n  \"results\": {\n    \"properties\": {},\n    \"eln\": {\n      \"sections\": [\n        \"ModelSystem\"\n      ]\n    }\n  }\n}\n</code></pre> <p>Assignment 3.2</p> <p><code>AtomicCell</code> should also contain information about the lattice vectors \u2014reciprocal lattice vectors are derived\u2014 and periodic boundary conditions. Add these to the instantiation, knowing that the lattice vectors fall under <code>&lt;structure&gt;&lt;crystal&gt;&lt;varray name=\"basis\" &gt;</code>. The boundary conditions, meanwhile, are always periodic in VASP.</p> Solution 3.2 <p>The new <code>ModelSystem().cell</code> attribute now reads as:</p> <p><pre><code>...\ncell=AtomicCell(\n    positions=xml_get(\"structure[@name='finalpos']/./varray[@name='positions']/v\")[0],\n    lattice_vectors=xml_get(\"structure//varray[@name='basis']/v\")[0],\n    periodic_boundary_conditions=[True] * 3,\n)\n</code></pre> </p> <p>This example shows a declarative approach to object instantiation: any quantity/subsection listed under a section in the schema can be directly passed to the constructor. Or course, the existence of section may be contingent on one and another.</p> <p>If no <code>finalpos</code> are extracted \u2014maybe due to a premature termination\u2014 neither should <code>model_system</code> be populated. In this case, it is better to set the section attribute after constructing the main skeleton.</p> Updating the Getter <p>Given our modifications, <code>xml_get</code> should now also be in charge of failure signaling. Depending on the type expected, we may use <code>None</code> or <code>[]</code>. To prevent <code>IndexError</code> when extracting, we also pass the array <code>slice(...)</code> along as an argument.</p> <pre><code>...\n\nclass VasprunXMLParser(MatchingParser):\n    def parse(\n    ...\n    )\n        ...\n        def xml_get(path: str, slicer=slice(0, 1), fallback=None):\n            try:\n                return xml_reader.parse(path)._results[path][slicer]\n            except KeyError:\n                return fallback\n\n\n        archive.data = Simulation(...)\n\n        if (\n            positions := xml_get(\n                \"structure[@name='finalpos']/./varray[@name='positions']/v\",\n                slice(None),\n                fallback=np.array([]),\n            )\n        ).any():\n            archive.data.model_system.append(\n                ModelSystem(\n                    cell=[AtomicCell(positions=np.float64(positions) * ureg.angstrom)]\n                )\n            )\n</code></pre> <p>Be mindful of the distinction between single/repeating sections, as they determine the interface, i.e. assignment/appending. NOMAD will raise errors like <code>Exception has occurred: TypeError - Subsection model_method repeats, but no list was given</code>.</p> <p>Another strategy is success short-circuiting, where code cycles through several trials until it encounters the first success. An real-world use case is how VASP uses different tags to distinguish density functionals at varying rungs on Jacob's Ladder (that require other routines), e.g. <code>LSDA</code>, <code>GGA</code>, <code>METAGGA</code>. PBE (<code>GGA</code> rung) is the defaults fall-back.</p> <p>Since all NOMAD sections are convertible to <code>dict</code>, one can generate a <code>get</code> chain trying out the least likely examples up until the default value (<code>PE</code> in this case).</p> <pre><code>...\n\nclass VasprunXMLParser(MatchingParser):\n    def parse(\n    ...\n    )\n        ...\n\n        archive.data = Simulation(\n            model_method=[\n                DFT(\n                    xc_functionals=[\n                        XCFunctional(\n                            libxc_name=self.convert_xc.get(\n                                xml_get(\n                                    \"///separator[@name='electronic exchange-correlation']/i[@name='LSDA']\"\n                                )[0],\n                                {},\n                            )\n                            .get(\n                                xml_get(\n                                    \"///separator[@name='electronic exchange-correlation']/i[@name='METAGGA']\"\n                                )[0],\n                                {},\n                            )\n                            .get(\n                                xml_get(\n                                    \"///separator[@name='electronic exchange-correlation']/i[@name='GGA']\"\n                                )[0],\n                                'PE',\n                            ),\n                        ),\n                    ],\n                ),\n            ],\n        )\n</code></pre> <p>Lastly, if at any point you require a derived property for your parser logic, you can <code>normalize()</code> the section and extract it. Just ensure to pass through the necessary information at the instantiation. The NOMAD base will anyhow invoke normalization, so do not feel responsible for normalizing the entire archive.</p> The Sense and Nonsense of Readers <p>In the examples above we fixated on the <code>XMLParser</code>, which really is more of a \"reader\", as it does not interface with NOMAD base directly. Instead, its only purpose is to make the file data accessible to Python.</p> <p>As such, a pragmatic attitude dictates that if you have more affinity for another reader, e.g. <code>lxml</code>, <code>BeautifulSoup</code>, <code>h5py</code> (demonstrated in the Schema Plugin part), etc. Just mind the dependencies. Indeed, <code>XMLParser</code> itself is based on <code>ElementTree</code>, and just extends the reading into <code>dict</code>. Feel free to similarly add logic to your own readers above the parsing class. This sums up the legacy approach.</p> <p>Note that the alternative approach shown in Mapping Annotations on the Schema side does require specific NOMAD base converters for interacting with the NOMAD schema.</p>"},{"location":"parser_plugins/#via-mapping","title":"Via Mapping","text":"<p>DISCLAIMER: the code and functionalities covered in this section are still under construction and only serve as an illustration of the underlying concepts. Stay tuned for updates.</p> <p>The second option is to have the instantiation run automatically. In that case, the mapping is added directly to our schema as an annotation. Leveraging this enhanced schema requires some new readers, like <code>mapping_parser.XMLParser</code> for the XML side and <code>MetainfoParser</code> for the schema side. The mapping is thus relegated to <code>schema_packages/vasp_schema.py</code>, leaving <code>parsers/xml_parser.py</code> as:</p> <pre><code>from structlog.stdlib import BoundLogger\nfrom nomad.datamodel.datamodel import EntryArchive\nfrom nomad.config import config\nfrom nomad.parsing import MatchingParser\nfrom nomad.parsing.file_parser.mapping_parser import MetainfoParser, XMLParser\nfrom nomad_parser_vasp.schema_packages.vasp_schema import Simulation\n\nconfiguration = config.get_plugin_entry_point(\n    'nomad_parser_vasp.parsers:xml_entry_point'\n)\n\nclass VasprunXMLParser(MatchingParser):\n    def parse(\n        self, mainfile: str, archive: EntryArchive, logger: BoundLogger,\n        child_archives: dict[str, EntryArchive] = None,\n    ) -&gt; None:\n        logger.info('VasprunXMLParser.parse', parameter=configuration.parameter)\n        data_parser = MetainfoParser(annotation_key='xml', data_object=Simulation())\n        XMLParser(filepath=mainfile).convert(data_parser)\n        archive.data = data_parser.data_object\n</code></pre> <p>Look at the power of this technique: this is the full parser! We only need to provide the root object, i.e. <code>Simulation</code>, from which to start the instantiation procedure.</p>"},{"location":"parser_plugins/#mapping-annotations-on-the-schema-side","title":"Mapping Annotations on the Schema side","text":"<p>DISCLAIMER: the code and functionalities covered in this section are still under construction and only serve as an illustration of the underlying concepts. Stay tuned for updates.</p> <p>So how do the annotations to the schema look like? Firstly, only attributes of <code>ArchiveSection</code>, i.e. <code>Quantity</code> (i.e. leaf nodes) or <code>SubSection</code> (i.e. branching nodes), have to be annotated. Adding annotations is as simple as extending a dictionary: <code>&lt;section_name&gt;.&lt;attribute_name&gt;.m_annotations[&lt;tag_name&gt;] = MappingAnnotationModel(...)</code>.</p> <p>The overall strategy is thus to annotate <code>path</code> mappings starting from <code>Simulation</code> and run over each <code>SubSection</code> up until reaching the corresponding <code>Quantity</code>. The most important part is for the target/NOMAD path to be fully traceable: any disconnections in a <code>path</code> will cut it out of the <code>archive</code>.</p> <pre><code>from nomad_simulations.schema_packages.general import Simulation, Program\nSimulation.m_def.m_annotations['xml'] = MappingAnnotationModel(path='modeling')\nSimulation.program.m_annotations['xml'] = MappingAnnotationModel(path='.generator')\nProgram.name.m_annotations['xml'] = MappingAnnotationModel(path='.i[?\"@name\"=\"program\"]')\n</code></pre> <p>Note how you can trace a continuous path down to <code>simulation.program.name</code>.</p>"},{"location":"parser_plugins/#from-text-to-hierarchies","title":"From Text to Hierarchies","text":"<p>When dealing with plain text, step 2 proceeds in two stages: reading in the file contents and making the <code>str</code> data more actionable in Python. To this end, we construct an intermediate tree format where we will map into the schema from.</p> <p>The tactic here is weave two classes, <code>TextParser</code> and <code>Quantity</code>, both from <code>text_parser.py</code>, in with each other. Always start with <code>TextParser</code> and use <code>Quantity.sub_parser</code> to go one level deeper. To obtain finally obtain the tree as a Python <code>dict</code>, apply <code>TextParser.to_dict()</code> to the root node.</p> <pre><code>txt_reader = TextParser(                # root node\n    quantities = [                      # level 1\n        Quantity(...),\n        Quantity(...),\n        Quantity(\n            ...,\n            sub_parser = TextParser(\n                quantities = [          # level 2\n                    Quantity(...),\n                ]\n            )\n        ),\n    ]\n)\n</code></pre> <p>Note that the regex patterns should always contain match groups, i.e. <code>()</code>, else no text is extracted. This is especially important for blocks, where the typical regex pattern has the form <code>r'&lt;re_header&gt;(?[\\s\\S]+)&lt;re_footer&gt;'</code> to match everything between the block header and footer.</p> Dissecting Tables <p>The typical approach to processing text tables is to match, in order:</p> <ol> <li>the table and extract (at least) the body.</li> <li>a standard line.</li> <li>the relevant column. </li> </ol> <p>Ensure that you toggle the <code>Quantity.repeats: Union[bool, int]</code> option to obtain a list of matches. </p> <p>The main concern is how to leverage the additional freedom of a third format. Our foremost advice is to:</p> <ol> <li>follow the order in which the data normally appears.</li> <li>use as similar as possible node names as in the file. If none are present, fall back on the NOMAD schema names.</li> <li>systematically break down blocks of text via the weaving technique.</li> <li>use the same node names when multiple versions exist. Maximize the common nodes and overall keep the alternatives as close as possible together.</li> </ol> Semantic Patterns <p>Modern text parsers come equipped with several common patterns to expedite the construction of complex patterns. Examples include <code>re_float</code> covering decimals and scientific notation, separators like <code>re_blank_line</code> or <code>re_eol</code>, and <code>re_non_greedy</code> for matching whole chunks of text, as shown above. The <code>capture(pattern)</code> function applies the match groups.</p>"},{"location":"schema_plugins/","title":"Part 4 - Extending NOMAD-Simulations to support custom methods and outputs","text":"<p>As you develop your parser, you may find that the <code>nomad-simulations</code> package does not include some relevant quantities for your particular use case. You can easily extend upon the schema by adding your own custom schema under the <code>schema_packages/</code> directory in your parser plugin. For this, you should utilize and build upon <code>nomad-simulations</code> for consistency and future compatibility or integration. Below (click to zoom) is a decision tree which illustrates the schema development process:</p> <p>From this schematic, we can identify 3 distinct uses or extensions of <code>nomad-simulations</code>:</p> <ol> <li> <p>direct use of existing schema section definitions (no extension)</p> <ul> <li>implementation already covered in Parser Plugins</li> </ul> </li> <li> <p>semantic extension</p> <ul> <li>create classes or sections that refine the context through inheritance from existing sections</li> <li>create brand new sections that widen the scope of the overall schema</li> </ul> </li> <li> <p>normalization functionalities</p> <ul> <li>the schema normalization functions can be leveraged to perform various tasks, simplifying the code within individual parsers while ensuring consistency and, thus, interoperability</li> </ul> </li> </ol> <p>The following demonstrates some simple examples of extending the <code>nomad-simulations</code> schema. More detailed documentation about writing schemas packages can be found in How to write a schema package within the general NOMAD documentation.</p> <p>To start developing a custom schema, create a python file for your schema, e.g., <code>&lt;parser_name&gt;_schema.py</code>, within your parser plugin project, under <code>schema_packages/</code>.</p> <p>Add the following imports to this file:</p> <pre><code>import numpy as np\nimport nomad_simulations\nfrom nomad.metainfo import Quantity, SubSection\n</code></pre> <p><code>SubSection</code> and <code>Quantity</code> are classes used to populate each section with specific metadata as demonstrated below.</p> ArchiveSection <p>All classes in <code>NOMAD</code> and <code>nomad-simulations</code> inherit from the most abstract class, <code>ArchiveSection</code>. This class is using the functionalities defined for a section in NOMAD, which is defined in <code>MSection</code>, as well as adding the <code>normalize()</code> function. This class function or method is important, as it is executed after parsing and allows to leverage several tasks out of parsing (as explained in point 3. above, and in Part II - Extra: The <code>normalize()</code> class function).</p>"},{"location":"schema_plugins/#extending-the-overarching-simulation-metadata","title":"Extending the overarching simulation metadata","text":"<p>Suppose you are developing a parser for a well-defined schema specified within the HDF5 file format. Importantly, the simulation data is harvested from the original simulation files and mapped into this schema/file format by some researcher. The overarching metadata in this file can be represented as</p> <pre><code>hdf5_schema\n    +-- version: Integer[3]\n    \\-- hdf5_generator\n    |    +-- name: String[]\n    |    +-- version: String[]\n    \\-- program\n        +-- name: String[]\n        +-- version: String[]\n</code></pre> HDF5 schema syntax <p><code>\\-- item</code> \u2013 An object within a group, that is either a dataset or a group. If it is a group itself, the objects within the group are indented by five spaces with respect to the group name.</p> <p><code>+-- attribute:</code> \u2013 An attribute, that relates either to a group or a dataset.</p> <p><code>\\-- data: &lt;type&gt;[dim1][dim2]</code> \u2013 A dataset with array dimensions dim1 by dim2 and of type , following the HDF5 Datatype classes. <p>Assignment 4.1</p> <p>Which quantities within this HDF5 schema can we store using existing sections within <code>nomad-simulations</code> and which require the creation of new schema sections? For the quantities that directly use the existing schema, write some code to demonstrate how your parser would populate the archive with this information.</p> Solution 4.1 <p>The program information can be stored under the existing <code>Program()</code> sections within <code>Simulation()</code>. However the HDF5 schema <code>version</code> and <code>hdf5_generator</code> information require some extensions to the schema.</p> <p>For populating the program information, the parser code would look something like this (NOTE: the specifics of how to access the HDF5 file is not essential to understand for the purposes here):</p> <pre><code>import h5py\nfrom nomad_simulations.schema_packages.general import Simulation, Program\n\nclass ParserName(MatchingParser):\n\n    def parse(\n        self,\n        mainfile: str,\n        archive: EntryArchive,\n        logger: BoundLogger,\n        child_archives: dict[str, EntryArchive] = None,\n    ) -&gt; None:\n\n        h5_data = h5py.File(mainfile, 'r')\n\n        simulation = Simulation()\n        simulation.program = Program(\n            name=h5_data['program'].attrs['name'],\n            version=h5_data['program'].attrs['version']\n        )\n\n        archive.data = simulation\n</code></pre> <p>The <code>hdf_generator</code> information is referring to a secondary software used to create the HDF5 file, so it makes sense to store this in a section very similar to <code>Program()</code>. Let's first take a look at the current <code>Program()</code> schema:</p> <pre><code>from nomad_simulations.general import Program\nprogram = Program()\nprint(program.m_def.all_quantities)\n</code></pre> <pre><code>{'name': nomad_simulations.general.Program.name:Quantity,\n 'datetime': nomad.datamodel.metainfo.basesections.BaseSection.datetime:Quantity,\n 'lab_id': nomad.datamodel.metainfo.basesections.BaseSection.lab_id:Quantity,\n 'description': nomad.datamodel.metainfo.basesections.BaseSection.description:Quantity,\n 'version': nomad_simulations.general.Program.version:Quantity,\n 'link': nomad_simulations.general.Program.link:Quantity,\n 'version_internal': nomad_simulations.general.Program.version_internal:Quantity,\n 'compilation_host': nomad_simulations.general.Program.compilation_host:Quantity}\n</code></pre> <p>Indeed, this section contains <code>name</code> and <code>version</code> information, along with other quantities that make sense in the context of a software which generates HDF5 files. So, in this case we can simply reuse this section and add a new subsection to <code>Simulation()</code>.</p> <p>Assignment 4.2</p> <p>Write down a new section that extends <code>Simulation()</code> to include a subsection <code>hdf_generator</code> of type <code>Program()</code>.</p> <p>HINT: Here is how the <code>program</code> section is defined within the <code>BaseSimulation()</code> section (parent of <code>Simulation()</code>) within the <code>nomad-simulations</code> package:</p> <pre><code>class BaseSimulation(Activity):\n    ...\n    program = SubSection(sub_section=Program.m_def, repeats=False)\n</code></pre> Solution 4.2 <p>We need to add a <code>Simulation()</code> section to <code>schema_packages/&lt;parser_name&gt;_schema.py</code> that inherits all the quantities and subsections from <code>nomad-simulation</code>'s <code>Simulation()</code> section, and additionally defines a subsection <code>hdf_generator</code>:</p> <pre><code>import nomad_simulations\n\nclass Simulation(nomad_simulations.schema_packages.general.Simulation):\n\n    hdf5_generator = SubSection(\n        sub_section=nomad_simulations.schema_packages.general.Program.m_def\n    )\n</code></pre> <p>The <code>sub_section</code> argument of <code>SubSection</code> specifies that this new section under <code>Simulation()</code> is of type <code>Program()</code> and will thus inherit all of its attributes.</p> <p>Now we can use this new section within our parser to store the hdf generator information:</p> <pre><code>from nomad_simulations.schema_packages.general import Program\nfrom &lt;parse_plugin&gt;.schema_packages.&lt;parser_name&gt;_schema.py import Simulation\n\nclass ParserName(MatchingParser):\n\n    def parse(\n        self,\n        mainfile: str,\n        archive: EntryArchive,\n        logger: BoundLogger,\n        child_archives: dict[str, EntryArchive] = None,\n    ) -&gt; None:\n\n        h5_data = h5py.File(mainfile, 'r')\n\n        simulation = Simulation()\n        simulation.hdf5_generator = Program(\n            name=h5_data['hdf_generator'].attrs['name'],\n            version=h5_data['hdf_generator'].attrs['version']\n        )\n\n        archive.data = simulation\n</code></pre> <p>Finally, we need to store the hdf5 schema version.</p> <p>Assignment 4.3</p> <p>Add a quantity <code>hdf5_schema_verion</code> with the appropriate <code>type</code>, <code>shape</code>, and <code>description</code> to your <code>Simulation()</code> class in <code>schema_packages/&lt;parser_name&gt;_schema.py</code>.</p> <p>HINT: You can browse some existing examples of quantity definitions in the <code>nomad-simulations</code> source code.</p> Solution 4.3 <pre><code>import nomad_simulations\n\nclass Simulation(nomad_simulations.schema_packages.general.Simulation):\n\n    hdf5_schema_version = Quantity(\n        type=np.int32,\n        shape=[3],\n        description=\"\"\"\n        Specifies the version of the HDF5 schema being followed, using sematic versioning.\n        \"\"\",\n    )\n</code></pre> <p>Since the HDF5 schema apparently uses semantic versioning, we define the <code>hdf5_schema_version</code> quantity as a list of 3 integers to ensure that the user provides the relevant information for this quantity.</p> <p>And in the parser:</p> <pre><code>from &lt;parse_plugin&gt;.schema_packages.&lt;parser_name&gt;_schema.py import Simulation\n\nclass ParserName(MatchingParser):\n\n    def parse(\n        self,\n        mainfile: str,\n        archive: EntryArchive,\n        logger: BoundLogger,\n        child_archives: dict[str, EntryArchive] = None,\n    ) -&gt; None:\n\n        h5_data = h5py.File(mainfile, 'r')\n\n        simulation = Simulation()\n        simulation.hdf5_schema_version = h5_data.attrs['version']\n\n        archive.data = simulation\n</code></pre>"},{"location":"schema_plugins/#extending-the-simulation-outputs","title":"Extending the simulation outputs","text":"<p>For this example, we will use the <code>nomad-vasp-parser</code> plugin that was referenced in the Parser Plugins. This plugin is currently under preliminary development. However we have prepared a branch with a minimal implementation for demonstration purposes. This branch follows the method of parsing described in Parser Plugins &gt; Via Instantiation.</p> <p>If you would like to get hands-on experience with the schema extension and parser execution, following the installation instructions below. Otherwise, you can read through the example below for a conceptual understanding.</p> <code>nomad-vasp-parser</code> installation instructions <p>To get started, clone the <code>nomad-vasp-parser</code> repository and checkout the branch <code>fairmat-tutorial-14</code> while creating your own local branch:</p> <pre><code>git clone https://github.com/FAIRmat-NFDI/nomad-parser-vasp.git\ncd nomad-parser-vasp/\ngit checkout origin/fairmat-tutorial-14 -b fairmat-tutorial-14\n</code></pre> <p>Now create a virtual environment and install the plugin (see the <code>READ.md</code> for full instructions):</p> <pre><code>python3 -m venv .pyenv\nsource .pyenv/bin/activate\npip install --upgrade pip\npip install uv\nuv pip install -e '.[dev]' --index-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi/simple\n</code></pre> <p>Open <code>pyproject.toml</code> within the root directory and confirm that you have the following dependencies:</p> <pre><code>dependencies = [\"nomad-lab&gt;=1.3.0\", \"nomad-simulations==0.0.3\"]\n</code></pre> <p>Now install the plugin:</p> <pre><code>uv pip install -e '.[dev]' --index-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi/simple\n</code></pre> <p>Briefly examine the <code>VASPXMLPaser()</code> in <code>nomad_vasp_parser/parsers/xml_parser.py</code> or below:</p> <code>VASPXMLParser.parse()</code> snippet <pre><code>def parse(\n    self,\n    mainfile: str,\n    archive: EntryArchive,\n    logger: BoundLogger,\n    child_archives: dict[str, EntryArchive] = None,\n) -&gt; None:\n    logger.info('VasprunXMLParser.parse', parameter=configuration.parameter)\n    xml_reader = XMLParser(mainfile=mainfile)  # XPath syntax\n\n    def xml_get(path: str, slicer=slice(0, 1), fallback=None):\n        try:\n            return xml_reader.parse(path)._results[path][slicer]\n        except KeyError:\n            return fallback\n\n    ####################################################\n    # Parse the basic program, method, and system data #\n    ####################################################\n    archive.data = Simulation(\n        program=Program(\n            name='VASP',\n            version=xml_get(\"//generator/i[@name='version']\")[0],\n        ),\n        model_method=[\n            DFT(\n                xc_functionals=[\n                    XCFunctional(\n                        libxc_name=self.convert_xc.get(\n                            xml_get(\n                                \"///separator[@name='electronic exchange-correlation']/i[@name='LSDA']\"\n                            ),\n                            {},\n                        )\n                        .get(\n                            xml_get(\n                                \"///separator[@name='electronic exchange-correlation']/i[@name='METAGGA']\"\n                            ),\n                            {},\n                        )\n                        .get(\n                            xml_get(\n                                \"///separator[@name='electronic exchange-correlation']/i[@name='GGA']\"\n                            ),\n                            'PE',\n                        ),\n                    ),\n                ],\n            ),\n        ],\n    )\n\n    if (\n        positions := xml_get(\n            \"structure[@name='finalpos']/./varray[@name='positions']/v\",\n            slice(None),\n            fallback=np.array([]),\n        )\n    ).any():\n        archive.data.model_system.append(\n            ModelSystem(cell=[AtomicCell(positions=positions)])\n        )\n</code></pre> <p>The current <code>VASPXMLParser.parse()</code> function populates the archive with a range of metadata including the program, method, and system data. We can view an example archive by running the parser in the terminal with some test data:</p> Running the parser <p>Let's see how to test our implementation within a Jupyter notebook, which will allow us to decouple the parsing from the normalization steps. First, from the terminal, install ipykernel in your environment, create a corresponding Jupyter kernel, and lauch a notebook:</p> <pre><code>pip install ipykernel\npython -m ipykernel install --user --name=vasp-plugin --display-name \"vasp-plugin\"\njupyter notebook\n</code></pre> <p>Open the notebook <code>tests/test_parse.ipynb</code> and select the <code>vasp-plugin</code> kernel in the top right.</p> <p>Execute the first 2 cells which follow the template provide in Parser Plugins &gt; Mainfile Interfacing &gt; Running your parser for parsing without normalization:</p> <pre><code>from nomad_parser_vasp.parsers.xml_parser import VasprunXMLParser\nfrom nomad.datamodel import EntryArchive\nfrom nomad.normalizing.metainfo import MetainfoNormalizer\nfrom nomad import utils\nlogger = utils.get_logger(__name__)\n</code></pre> <pre><code>path = './data/'\np = VasprunXMLParser()\na = EntryArchive()\np.parse(path + 'vasprun.xml.relax', a, logger=logger)\n</code></pre> <p>Execute the third cell to examine the archive in dictionary format: <pre><code>a.m_to_dict()\n</code></pre></p> <p>The output should be: <pre><code>{'data': {'m_def': 'nomad_simulations.schema_packages.general.Simulation',\n'program': {'name': 'VASP', 'version': '5.3.2'},\n'model_system': [{'datetime': '2024-08-15T15:55:41.135408+00:00',\n    'branch_depth': 0,\n    'cell': [{'m_def': 'nomad_simulations.schema_packages.model_system.AtomicCell',\n    'name': 'AtomicCell',\n    'positions': [[0.0, 0.0, 0.0], [0.500001, 0.500001, 0.500001]],\n    'periodic_boundary_conditions': [False, False, False]}]}],\n'model_method': [{'m_def': 'nomad_simulations.schema_packages.model_method.DFT',\n    'xc_functionals': [{'libxc_name': 'PE'}]}]},\n'results': {'eln': {'sections': ['ModelSystem']}}}\n</code></pre></p> <p>Now take another look at the next section of the <code>VASPXMLParser.parse()</code> code in your branch. You will find that 3 energy quantities have been extracted from the xml file and placed into variables with the appropriate units assigned:</p> <pre><code>#####################################################\n# Get the energy data from the raw simulation files #\n#####################################################\ntotal_energy = xml_get(\"i[@name='e_fr_energy']\", slice(-2, -1))\ntotal_energy = total_energy[0] * ureg.eV if total_energy else None\nhartreedc = xml_get(\"i[@name='hartreedc']\", slice(-2, -1))\nhartreedc = hartreedc[0] * ureg.eV if hartreedc else None\nxcdc = xml_get(\"i[@name='XCdc']\", slice(-2, -1))\nxcdc = xcdc[0] * ureg.eV if xcdc else None\n</code></pre> <p>There already exists a section for total energy within <code>nomad-simulations</code>:</p> <pre><code>class BaseEnergy(PhysicalProperty):\n    \"\"\"\n    Abstract class used to define a common `value` quantity with the appropriate units\n    for different types of energies, which avoids repeating the definitions for each\n    energy class.\n    \"\"\"\n\n    value = Quantity(\n        type=np.float64,\n        unit='joule',\n        description=\"\"\"\n        \"\"\",\n    )\n\n    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -&gt; None:\n        super().normalize(archive, logger)\n\nclass EnergyContribution(BaseEnergy, PropertyContribution):\n    ...\n\nclass TotalEnergy(BaseEnergy):\n    \"\"\"\n    The total energy of a system. `contributions` specify individual energetic\n    contributions to the `TotalEnergy`.\n    \"\"\"\n\n    contributions = SubSection(sub_section=EnergyContribution.m_def, repeats=True)\n\n    def __init__(\n        self, m_def: 'Section' = None, m_context: 'Context' = None, **kwargs\n    ) -&gt; None:\n        super().__init__(m_def, m_context, **kwargs)\n        self.name = self.m_def.name\n\n    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -&gt; None:\n        super().normalize(archive, logger)\n</code></pre> <p>The <code>BaseEnergy</code> section simply defines a <code>PhysicalProperty</code> with the appropriate units for its <code>value</code>. Total energy also contains a subsection <code>contributions</code> where other energies contributing to the total energy can be stored. These contribution are of type <code>EnergyContributions</code>. In short, this is a section that enables the linking of these energy contributions to specific components of the corresponding method. However, this is not important for the present example.</p> <p>Assignment 4.4</p> <p>The parsed hartreedc and xcdc energies are both classified as \"double-counting energies\". Add 3 new sections in <code>schema_packages/vasp_parser_schema.py</code>: one for each of the parsed energies and then an additional abstract class that each of these energy sections inherits from.</p> Solution 4.4 <pre><code>import nomad_simulations\nfrom nomad.metainfo import MEnum, Quantity\nfrom nomad_simulations.schema_packages.properties.energies import EnergyContribution\n\nclass DoubleCountingEnergy(EnergyContribution):\n    type = Quantity(\n        type=MEnum('double_counting'),\n    )\n\n    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -&gt; None:\n        super().normalize(archive, logger)\n\n        if not self.type:\n            self.type = 'double_counting'\n\n\nclass HartreeDCEnergy(DoubleCountingEnergy):\n    def __init__(\n        self, m_def: 'Section' = None, m_context: 'Context' = None, **kwargs\n    ) -&gt; None:\n        super().__init__(m_def, m_context, **kwargs)\n        self.name = self.m_def.name\n\n    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -&gt; None:\n        super().normalize(archive, logger)\n\n\nclass XCdcEnergy(DoubleCountingEnergy):\n    def __init__(\n        self, m_def: 'Section' = None, m_context: 'Context' = None, **kwargs\n    ) -&gt; None:\n        super().__init__(m_def, m_context, **kwargs)\n        self.name = self.m_def.name\n\n    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -&gt; None:\n        super().normalize(archive, logger)\n</code></pre> <p><code>DoubleCountingEnergy</code> inherits from <code>EnergyContribution</code> so that we can add each of the parsed energies to <code>total_energy.contributions</code>. Then each of the newly defined specific energy sections inherits from <code>DoubleCountingEnergy</code>. The <code>__init__</code> function sets the name of each contribution section to the name of the corresponding class.</p> <p>We can go one step further and utilize the normalization function of the <code>DoubleCountingEnergy</code> section to set the <code>type</code> quantity of the <code>PhysicalProperty()</code> section as a characterization for each of the child sections. We have also overwritten the <code>type</code> quantity to be an <code>MEnum('double_counting')</code>, which will trigger an error to be thrown if the parser sets <code>energy_class.type</code> to anything other than <code>double_counting</code> for any section that inherits from <code>DoubleCountingEnergy</code>.</p> <p>NOTE: In practice we also need to add detailed descriptions for each section!</p> <p>The normalization function within each schema section definition allows us to perform consistent operations when particular sections are instantiated or particular quantities are set. This removes complexity from individual parsers and ensures consistency and, thus, interoperability.</p> <p>Before actually populating these new energy contributions in the parser, let's consider one additional normalization functionality.</p> <p>Assignment 4.5</p> <p>Implement a new section for total energy that extends the <code>nomad-simulations</code>' <code>TotalEnergy()</code> section to include a normalization function that: 1. calculates the difference between the total energy and each of its contributions, and 2. stores this remainder contribution as an additional contribution to the total energy called <code>UnknownEnergy</code>. Don't forget to create the appropriate section for this new type of energy contribution.</p> <p>Challenge: Make sure your function covers the following 3 cases: 1. the <code>UnknownEnergy</code> contribution is instantiated and its value populated within the parser, 2. the <code>UnknownEnergy</code> class is instantiated in the parser but no value is give, and 3. no <code>UnknownEnergy</code> contribution is instantiated during parsing.</p> Solution 4.5 <pre><code>class UnknownEnergy(EnergyContribution):\n    def __init__(\n        self, m_def: 'Section' = None, m_context: 'Context' = None, **kwargs\n    ) -&gt; None:\n        super().__init__(m_def, m_context, **kwargs)\n        self.name = self.m_def.name\n\n    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -&gt; None:\n        super().normalize(archive, logger)\n\n\nclass TotalEnergy(nomad_simulations.schema_packages.properties.TotalEnergy):\n    def normalize(self, archive: 'EntryArchive', logger: 'BoundLogger') -&gt; None:\n        super().normalize(archive, logger)\n\n        if not self.value:\n            return\n        if not self.contributions:\n            return\n\n        value = self.value\n        unknown_exists = False\n        unknown_has_value = False\n        i_unknown = None\n        for i_cont, contribution in enumerate(self.contributions):\n            if contribution.name == 'UnknownEnergy':\n                unknown_exists = True\n                i_unknown = i_cont\n                unknown_has_value = True if contribution.value else False\n\n            if not contribution.value:\n                continue\n\n            value -= contribution.value\n\n        if unknown_exists:\n            if not unknown_has_value:\n                self.contributions[i_unknown].value = value\n        else:\n            self.contributions.append(UnknownEnergy(value=value))\n            self.contributions[-1].normalize(archive, logger)\n</code></pre> <p>Assignment 4.6</p> <p>Now write the appropriate code for the <code>VasprunXMLParser()</code> to populate the energies.</p> Solution 4.6 <pre><code>from nomad_simulations.schema_packages.outputs import Outputs\nfrom nomad_parser_vasp.schema_packages.vasp_schema import (\n    HartreeDCEnergy,\n    TotalEnergy,\n    XCdcEnergy,\n)\n\nclass VasprunXMLParser(MatchingParser):\n    convert_xc: dict[str, str] = {\n        '--': 'GGA_XC_PBE',\n        'PE': 'GGA_XC_PBE',\n    }\n\n    def parse(\n        self,\n        mainfile: str,\n        archive: EntryArchive,\n        logger: BoundLogger,\n        child_archives: dict[str, EntryArchive] = None,\n    ) -&gt; None:\n        logger.info('VasprunXMLParser.parse', parameter=configuration.parameter)\n        xml_reader = XMLParser(mainfile=mainfile)  # XPath syntax\n\n        def xml_get(path: str, slicer=slice(0, 1), fallback=None):\n            try:\n                return xml_reader.parse(path)._results[path][slicer]\n            except KeyError:\n                return fallback\n\n        ...\n        ...\n\n    ####################################################\n    # Create the outputs section, populate it with the #\n    # parsed energies, and add it to the archive       #\n    ####################################################\n    output = Outputs()\n    archive.data.outputs.append(output)\n    output.total_energy.append(TotalEnergy(value=total_energy))\n    output.total_energy[0].contributions.append(HartreeDCEnergy(value=hartreedc))\n    output.total_energy[0].contributions.append(XCdcEnergy(value=xcdc))\n</code></pre> <p>Notice that although <code>TotalEnergy()</code> exists within <code>nomad-simulations</code>, we need to import this class from the newly created schema to take advantage of our newly created normalization function.</p> <p>Now let's test our implementation. Follow the steps under \"Running your parser\" above, using the test notebook provided (<code>tests/test_parser.ipynb</code>). Note that you need to restart your kernel every time you make changes to the plugin code. After successful parsing, check the energy entries within the archive.</p> Checking the populated archive <pre><code>total_energy = a.data.outputs[0].total_energy[0]\nprint(total_energy.name)\nprint(total_energy.value)\nprint(total_energy.contributions)\n</code></pre> <pre><code>TotalEnergy\n-1.1442321664199474e-18 joule\n[HartreeDCEnergy:HartreeDCEnergy(name, value), XCdcEnergy:XCdcEnergy(name, value)]\n</code></pre> <pre><code>hartreedc = total_energy.contributions[0]\nprint(hartreedc.name)\nprint(hartreedc.value)\n</code></pre> <pre><code>HartreeDCEnergy\n-6.432015131607956e-17 joule\n</code></pre> <pre><code>xcdc = total_energy.contributions[1]\nprint(xcdc.name)\nprint(xcdc.value)\n</code></pre> <pre><code>XCdcEnergy\n-7.660195079365588e-18 joule\n</code></pre> <p>Now, rerun the parsing, followed by schema normalization (i.e., only normalization functions defined within the schema classes are being executed. Higher-level NOMAD normalization is ignored):</p> <pre><code>path = './data/'\np = VasprunXMLParser()\na = EntryArchive()\np.parse(path + 'vasprun.xml.relax', a, logger=logger)\n\nMetainfoNormalizer().normalize(archive=a)\n</code></pre> <p>NOTE: There will be some messages warning about certain sections not being normalized. This is simply because our parsing was incomplete. Please ignore these.</p> Checking the populated archive <pre><code>total_energy = a.data.outputs[0].total_energy[0]\nprint(total_energy.name)\nprint(total_energy.value)\nprint(total_energy.contributions)\n</code></pre> <pre><code>TotalEnergy\n-1.1442321664199474e-18 joule\n[HartreeDCEnergy:HartreeDCEnergy(name, type, is_derived, variables, value), XCdcEnergy:XCdcEnergy(name, type, is_derived, variables, value), UnknownEnergy:UnknownEnergy(name, is_derived, value)]\n</code></pre> <pre><code>hartreedc = total_energy.contributions[0]\nprint(hartreedc.name)\nprint(hartreedc.type)\nprint(hartreedc.value)\n</code></pre> <pre><code>HartreeDCEnergy\ndouble_counting\n-6.432015131607956e-17 joule\n</code></pre> <pre><code>xcdc = total_energy.contributions[1]\nprint(xcdc.name)\nprint(xcdc.type)\nprint(xcdc.value)\n</code></pre> <pre><code>XCdcEnergy\ndouble_counting\n-7.660195079365588e-18 joule\n</code></pre> <pre><code>unknown = total_energy.contributions[2]\nprint(unknown.name)\nprint(unknown.value)\n</code></pre> <pre><code>UnknownEnergy\n7.08361142290252e-17 joule\n</code></pre> <p>Our normalizations worked! We now have additional quantities <code>type</code> defined under our double counting contributions, and we have the new <code>UnknownEnergy</code> contribution.</p> <p>To make sure that our implementation is robust, we should consider different possible use cases of <code>UnknownEnergy</code>. We have already verified that the normalization works properly if the parser writer does not know about this energy contribution. But what if they see this section in the schema and populate the archive with an instance of the class but no value:</p> <pre><code># Case 2: Add UnknownEnergy to contribution list in the parser but without a value\nfrom nomad_parser_vasp.schema_packages.vasp_schema import UnknownEnergy\n\noutput.total_energy[0].contributions.append(UnknownEnergy(value=None))\n# Expected Results: UnknownEnergy value is calculated by the normalizer and placed into this section\n</code></pre> Checking the populated archive <p>After restarting your notebook kernel and rerunning the parsing and normalization as before:</p> <pre><code>total_energy = a.data.outputs[0].total_energy[0]\nprint(total_energy.name)\nprint(total_energy.value)\nprint(total_energy.contributions)\n</code></pre> <pre><code>TotalEnergy\n-1.1442321664199474e-18 joule\n[HartreeDCEnergy:HartreeDCEnergy(name, type, is_derived, variables, value), XCdcEnergy:XCdcEnergy(name, type, is_derived, variables, value), UnknownEnergy:UnknownEnergy(name, is_derived, variables, value)]\n</code></pre> <pre><code>hartreedc = total_energy.contributions[0]\nprint(hartreedc.name)\nprint(hartreedc.type)\nprint(hartreedc.value)\n</code></pre> <pre><code>HartreeDCEnergy\ndouble_counting\n-6.432015131607956e-17 joule\n</code></pre> <pre><code>xcdc = total_energy.contributions[1]\nprint(xcdc.name)\nprint(xcdc.type)\nprint(xcdc.value)\n</code></pre> <pre><code>XCdcEnergy\ndouble_counting\n-7.660195079365588e-18 joule\n</code></pre> <pre><code>unknown = total_energy.contributions[2]\nprint(unknown.name)\nprint(unknown.value)\n</code></pre> <pre><code>UnknownEnergy\n7.08361142290252e-17 joule\n</code></pre> <p>Finally, what about the case where the parser developer populates the unknown contribution manually:</p> <pre><code># Case 3: Add UnknownEnergy to contribution list in the parser with a value\nfrom nomad_parser_vasp.schema_packages.vasp_schema import UnknownEnergy\n\noutput.total_energy[0].contributions.append(\n    UnknownEnergy(value=(total_energy - 2 * hartreedc - xcdc))\n)\n# Expected Results: normalizer does not change the value of UnknownEnergy\n# (for testing purposes we subtract double the hartreedc value)\n</code></pre> Checking the populated archive <p>Make sure to comment out Case 2 and restart your notebook kernel. After rerunning the parsing and normalization as before:</p> <pre><code>total_energy = a.data.outputs[0].total_energy[0]\nprint(total_energy.name)\nprint(total_energy.value)\nprint(total_energy.contributions)\n</code></pre> <pre><code>TotalEnergy\n-1.1442321664199474e-18 joule\n[HartreeDCEnergy:HartreeDCEnergy(name, type, is_derived, variables, value), XCdcEnergy:XCdcEnergy(name, type, is_derived, variables, value), UnknownEnergy:UnknownEnergy(name, is_derived, variables, value)]\n</code></pre> <pre><code>hartreedc = total_energy.contributions[0]\nprint(hartreedc.name)\nprint(hartreedc.type)\nprint(hartreedc.value)\n</code></pre> <pre><code>HartreeDCEnergy\ndouble_counting\n-6.432015131607956e-17 joule\n</code></pre> <pre><code>xcdc = total_energy.contributions[1]\nprint(xcdc.name)\nprint(xcdc.type)\nprint(xcdc.value)\n</code></pre> <pre><code>XCdcEnergy\ndouble_counting\n-7.660195079365588e-18 joule\n</code></pre> <pre><code>unknown = total_energy.contributions[2]\nprint(unknown.name)\nprint(unknown.value)\n</code></pre> <pre><code>UnknownEnergy\n1.3515626554510475e-16 joule\n</code></pre> <p>Fully implemented solutions to these exercises and the corresponding results can be found in the plugin repo: <code>src/parsers/xml_parser_all_solutions.py</code>, <code>src/schema_packages/vasp_schema_all_solutions.py</code>, <code>tests/test_parse_all_solutions.pdf</code>.</p>"}]}